# 对比学习（Contrastive Learning）详解

## 一、什么是对比学习

### 1.1 核心思想

对比学习是一种**自监督/弱监督**学习范式，核心思想是：

> 通过对比正样本对和负样本对，学习有意义的表示

**目标**：
- 正样本对（相似样本）：拉近它们的表示
- 负样本对（不相似样本）：推远它们的表示

```
正样本对：(锚点, 正样本) → 距离小
负样本对：(锚点, 负样本) → 距离大
```

### 1.2 与其他学习范式的对比

| 范式 | 监督信号 | 示例 |
|------|----------|------|
| 监督学习 | 标签 | 分类、回归 |
| 自监督学习 | 数据本身 | MLM、对比学习 |
| 对比学习 | 正负样本对 | SimCLR、MoCo |

### 1.3 对比学习的优势

1. **不需要大量标注数据**：可以从数据本身构造正负样本
2. **学到的表示更通用**：不针对特定任务
3. **迁移能力强**：预训练后可用于多种下游任务

---

## 二、对比学习的数学框架

### 2.1 问题设定

给定：
- 锚点样本 $x$
- 正样本 $x^+$（与 $x$ 相似）
- 负样本集合 $\{x_1^-, x_2^-, ..., x_K^-\}$（与 $x$ 不相似）
- 编码器 $f_\theta$

目标：学习 $f_\theta$，使得：
$$\text{sim}(f_\theta(x), f_\theta(x^+)) \gg \text{sim}(f_\theta(x), f_\theta(x_i^-))$$

### 2.2 InfoNCE Loss

最常用的对比学习损失函数：

$$\mathcal{L}_{InfoNCE} = -\log \frac{\exp(\text{sim}(z, z^+)/\tau)}{\exp(\text{sim}(z, z^+)/\tau) + \sum_{i=1}^{K} \exp(\text{sim}(z, z_i^-)/\tau)}$$

其中：
- $z = f_\theta(x)$：锚点的表示
- $z^+ = f_\theta(x^+)$：正样本的表示
- $z_i^- = f_\theta(x_i^-)$：负样本的表示
- $\tau$：温度参数
- $\text{sim}(\cdot, \cdot)$：相似度函数（通常是余弦相似度）

### 2.3 InfoNCE 的直觉理解

将 InfoNCE 改写为 softmax 形式：

$$\mathcal{L} = -\log \frac{\exp(s^+/\tau)}{\sum_{j} \exp(s_j/\tau)}$$

这是一个 **(K+1) 分类问题**：
- 从 K+1 个样本中识别出正样本
- 最小化损失 = 最大化正样本的概率

### 2.4 温度参数 τ 的作用

$$\text{softmax}(s_i/\tau) = \frac{\exp(s_i/\tau)}{\sum_j \exp(s_j/\tau)}$$

| τ 值 | 效果 |
|------|------|
| τ → 0 | 分布趋向 one-hot，只关注最相似的 |
| τ → ∞ | 分布趋向均匀，所有样本权重相同 |
| τ 小 | 对 hard negatives 更敏感 |
| τ 大 | 更平滑，训练更稳定 |

**经验值**：τ ∈ [0.05, 0.1] 通常效果较好

### 2.5 InfoNCE 与互信息

InfoNCE 是互信息的下界：

$$I(X; Y) \geq \log(K) - \mathcal{L}_{InfoNCE}$$

其中 $I(X; Y)$ 是 $X$ 和 $Y$ 之间的互信息。

**直觉**：最小化 InfoNCE 损失 ≈ 最大化正样本对之间的互信息

---

## 三、正负样本的构造

### 3.1 图像领域的数据增强

**SimCLR 的增强策略**：

```python
def simclr_augment(image):
    """
    对同一张图片应用不同的增强，得到正样本对
    """
    transforms = [
        RandomResizedCrop(size=224),
        RandomHorizontalFlip(),
        ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        RandomGrayscale(p=0.2),
        GaussianBlur(kernel_size=23),
    ]
    
    # 应用随机增强
    augmented = apply_random_transforms(image, transforms)
    return augmented

# 正样本对：同一张图片的两个不同增强版本
x1 = simclr_augment(image)
x2 = simclr_augment(image)
# (x1, x2) 是正样本对
```

### 3.2 文本领域的正样本构造

#### 方法1：数据增强

```python
def text_augment(text):
    """文本增强"""
    methods = [
        back_translation,      # 回译
        synonym_replacement,   # 同义词替换
        random_deletion,       # 随机删除
        random_swap,          # 随机交换
    ]
    return random.choice(methods)(text)
```

#### 方法2：自然正样本对

| 数据类型 | 正样本对 |
|----------|----------|
| 问答数据 | (问题, 答案) |
| 搜索日志 | (查询, 点击文档) |
| 平行语料 | (英文句, 中文句) |
| NLI 数据 | (前提, 蕴含假设) |
| 标题-正文 | (标题, 正文) |

#### 方法3：Dropout 作为增强

SimCSE 的发现：同一句子过两次 Dropout 就是好的正样本对

```python
def simcse_positive_pair(text, model):
    """
    SimCSE: 用 Dropout 构造正样本
    """
    # 同一句子，两次前向传播（不同的 Dropout mask）
    model.train()  # 启用 Dropout
    z1 = model(text)
    z2 = model(text)  # 不同的 Dropout → 不同的表示
    return z1, z2
```

### 3.3 负样本的构造

#### In-batch Negatives

同一 batch 内的其他样本作为负样本：

```python
def in_batch_negatives(batch_embeddings):
    """
    batch_embeddings: [batch_size, dim]
    每个样本的负样本是 batch 内的其他所有样本
    """
    batch_size = batch_embeddings.size(0)
    
    # 相似度矩阵
    sim_matrix = torch.matmul(batch_embeddings, batch_embeddings.T)
    # sim_matrix[i][j] = sim(sample_i, sample_j)
    
    # 对角线是正样本（自己和自己）
    # 非对角线是负样本
    
    return sim_matrix
```

**优点**：高效，不需要额外采样
**缺点**：负样本可能太简单

#### Hard Negatives

精心挑选的难负样本：

```python
def mine_hard_negatives(query, positive, corpus, model, top_k=10):
    """
    挖掘 hard negatives
    """
    query_emb = model.encode(query)
    
    # 计算与所有文档的相似度
    scores = []
    for doc in corpus:
        if doc == positive:
            continue
        doc_emb = model.encode(doc)
        score = cosine_similarity(query_emb, doc_emb)
        scores.append((doc, score))
    
    # 选择相似度高但不是正样本的文档
    scores.sort(key=lambda x: -x[1])
    hard_negatives = [doc for doc, _ in scores[:top_k]]
    
    return hard_negatives
```

**Hard Negative 的来源**：
1. BM25 检索的 top-k 非相关文档
2. 当前模型检索的 false positives
3. 其他 query 的正样本

#### 负样本数量的影响

$$\mathcal{L} = -\log \frac{\exp(s^+/\tau)}{\exp(s^+/\tau) + \sum_{i=1}^{K} \exp(s_i^-/\tau)}$$

- K 越大，任务越难，学到的表示越好
- 但 K 太大，计算成本高
- 经验值：K ∈ [256, 65536]

---

## 四、经典对比学习方法

### 4.1 SimCLR

**架构**：

```
Image → Encoder → Projection Head → z
                      ↓
              对比学习损失
```

**关键设计**：
1. 强数据增强
2. 非线性 Projection Head
3. 大 batch size（4096-8192）

```python
class SimCLR(nn.Module):
    def __init__(self, encoder, projection_dim=128):
        super().__init__()
        self.encoder = encoder
        self.projection = nn.Sequential(
            nn.Linear(encoder.output_dim, encoder.output_dim),
            nn.ReLU(),
            nn.Linear(encoder.output_dim, projection_dim)
        )
    
    def forward(self, x):
        h = self.encoder(x)  # 表示
        z = self.projection(h)  # 投影
        return h, z
    
    def contrastive_loss(self, z1, z2, temperature=0.5):
        """
        z1, z2: [batch_size, projection_dim]
        """
        batch_size = z1.size(0)
        
        # 归一化
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)
        
        # 拼接
        z = torch.cat([z1, z2], dim=0)  # [2*batch_size, dim]
        
        # 相似度矩阵
        sim = torch.matmul(z, z.T) / temperature  # [2*batch_size, 2*batch_size]
        
        # 正样本对的位置
        # (i, i+batch_size) 和 (i+batch_size, i) 是正样本对
        labels = torch.cat([
            torch.arange(batch_size, 2*batch_size),
            torch.arange(batch_size)
        ]).to(z.device)
        
        # 移除对角线（自己和自己）
        mask = torch.eye(2*batch_size, dtype=torch.bool).to(z.device)
        sim = sim.masked_fill(mask, -float('inf'))
        
        # 交叉熵损失
        loss = F.cross_entropy(sim, labels)
        
        return loss
```

### 4.2 MoCo（Momentum Contrast）

**解决的问题**：SimCLR 需要大 batch size，内存消耗大

**核心思想**：用动量更新的队列存储负样本

```
Query Encoder (θ) → q
Key Encoder (θ_k) → k  (动量更新)
                    ↓
              负样本队列 [k1, k2, ..., kK]
```

```python
class MoCo(nn.Module):
    def __init__(self, encoder, dim=128, K=65536, m=0.999, T=0.07):
        """
        K: 队列大小
        m: 动量系数
        T: 温度
        """
        super().__init__()
        self.K = K
        self.m = m
        self.T = T
        
        # Query encoder
        self.encoder_q = encoder
        # Key encoder (动量更新)
        self.encoder_k = copy.deepcopy(encoder)
        
        # 冻结 key encoder
        for param in self.encoder_k.parameters():
            param.requires_grad = False
        
        # 负样本队列
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = F.normalize(self.queue, dim=0)
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))
    
    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        """动量更新 key encoder"""
        for param_q, param_k in zip(
            self.encoder_q.parameters(), 
            self.encoder_k.parameters()
        ):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)
    
    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        """更新队列"""
        batch_size = keys.shape[0]
        ptr = int(self.queue_ptr)
        
        # 替换队列中的旧样本
        self.queue[:, ptr:ptr + batch_size] = keys.T
        ptr = (ptr + batch_size) % self.K
        self.queue_ptr[0] = ptr
    
    def forward(self, x_q, x_k):
        """
        x_q: query 样本
        x_k: key 样本（正样本）
        """
        # Query
        q = self.encoder_q(x_q)
        q = F.normalize(q, dim=1)
        
        # Key
        with torch.no_grad():
            self._momentum_update_key_encoder()
            k = self.encoder_k(x_k)
            k = F.normalize(k, dim=1)
        
        # 正样本相似度
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        
        # 负样本相似度（从队列中）
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])
        
        # logits
        logits = torch.cat([l_pos, l_neg], dim=1) / self.T
        
        # 标签：正样本在第 0 位
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)
        
        # 更新队列
        self._dequeue_and_enqueue(k)
        
        return F.cross_entropy(logits, labels)
```

### 4.3 SimCSE（文本对比学习）

**无监督 SimCSE**：

```python
def unsupervised_simcse_loss(model, sentences, temperature=0.05):
    """
    用 Dropout 作为数据增强
    """
    # 同一句子过两次模型（不同 Dropout）
    model.train()
    z1 = model(sentences)
    z2 = model(sentences)
    
    # 余弦相似度
    z1 = F.normalize(z1, dim=1)
    z2 = F.normalize(z2, dim=1)
    sim = torch.matmul(z1, z2.T) / temperature
    
    # 对角线是正样本
    labels = torch.arange(sim.size(0)).to(sim.device)
    
    loss = F.cross_entropy(sim, labels)
    return loss
```

**有监督 SimCSE**：

使用 NLI 数据，(前提, 蕴含) 作为正样本，(前提, 矛盾) 作为 hard negative

```python
def supervised_simcse_loss(model, premises, entailments, contradictions, temperature=0.05):
    """
    premises: 前提句子
    entailments: 蕴含句子（正样本）
    contradictions: 矛盾句子（hard negative）
    """
    z_p = model(premises)
    z_e = model(entailments)
    z_c = model(contradictions)
    
    # 归一化
    z_p = F.normalize(z_p, dim=1)
    z_e = F.normalize(z_e, dim=1)
    z_c = F.normalize(z_c, dim=1)
    
    # 正样本相似度
    pos_sim = torch.sum(z_p * z_e, dim=1, keepdim=True) / temperature
    
    # hard negative 相似度
    hard_neg_sim = torch.sum(z_p * z_c, dim=1, keepdim=True) / temperature
    
    # in-batch negatives
    neg_sim = torch.matmul(z_p, z_e.T) / temperature
    
    # 拼接
    logits = torch.cat([pos_sim, hard_neg_sim, neg_sim], dim=1)
    
    # 正样本在第 0 位
    labels = torch.zeros(logits.size(0), dtype=torch.long).to(logits.device)
    
    loss = F.cross_entropy(logits, labels)
    return loss
```

---

## 五、对比学习的理论分析

### 5.1 对齐与均匀性

Wang & Isola (2020) 提出对比学习学到的表示应该满足：

**对齐性（Alignment）**：正样本对的表示应该接近

$$\mathcal{L}_{align} = \mathbb{E}_{(x, x^+) \sim p_{pos}} ||f(x) - f(x^+)||^2$$

**均匀性（Uniformity）**：表示应该均匀分布在超球面上

$$\mathcal{L}_{uniform} = \log \mathbb{E}_{x, y \sim p_{data}} e^{-2||f(x) - f(y)||^2}$$

InfoNCE 隐式地优化了这两个目标。

### 5.2 表示坍缩问题

**问题**：如果所有样本都映射到同一个点，损失也可以很低

**解决方案**：
1. 负样本：推远不同样本的表示
2. 正则化：如 BatchNorm、LayerNorm
3. 停止梯度：如 BYOL、SimSiam

### 5.3 Hard Negatives 的重要性

**梯度分析**：

$$\frac{\partial \mathcal{L}}{\partial z} = -\frac{1}{\tau}(z^+ - \sum_i p_i z_i^-)$$

其中 $p_i = \frac{\exp(z \cdot z_i^- / \tau)}{\sum_j \exp(z \cdot z_j^- / \tau)}$

- 简单负样本：$p_i$ 很小，贡献的梯度小
- Hard negatives：$p_i$ 较大，贡献的梯度大

**结论**：Hard negatives 提供更有效的学习信号

---

## 六、对比学习在 RAG 中的应用

### 6.1 训练 Embedding 模型

```python
def train_embedding_model(model, train_data, epochs=10):
    """
    train_data: [(query, positive_doc, negative_docs), ...]
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    
    for epoch in range(epochs):
        for query, pos_doc, neg_docs in train_data:
            # 编码
            q_emb = model.encode(query)
            pos_emb = model.encode(pos_doc)
            neg_embs = [model.encode(neg) for neg in neg_docs]
            
            # InfoNCE 损失
            pos_score = torch.dot(q_emb, pos_emb)
            neg_scores = torch.stack([torch.dot(q_emb, neg) for neg in neg_embs])
            
            logits = torch.cat([pos_score.unsqueeze(0), neg_scores]) / 0.05
            labels = torch.tensor([0])
            
            loss = F.cross_entropy(logits, labels)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

### 6.2 多阶段训练

```
阶段1：大规模弱监督预训练
  - 数据：网页标题-正文、锚文本-目标页面
  - 负样本：In-batch negatives
  - 目标：学习基础语义表示

阶段2：高质量数据微调
  - 数据：人工标注的相似度数据
  - 负样本：BM25 hard negatives
  - 目标：提升相似度判断能力

阶段3：任务特定微调
  - 数据：目标领域的数据
  - 负样本：模型自挖掘的 hard negatives
  - 目标：适应特定领域
```

### 6.3 实践建议

| 方面 | 建议 |
|------|------|
| 温度 | 0.05-0.1 |
| Batch size | 越大越好（至少 256） |
| 负样本数 | 7-15 个 hard negatives |
| 学习率 | 1e-5 到 5e-5 |
| 训练轮数 | 3-10 epochs |
