# BM25 算法详解

## 一、信息检索基础

### 1.1 问题定义

给定：
- 文档集合 $\mathcal{D} = \{d_1, d_2, ..., d_N\}$
- 查询 $q = \{t_1, t_2, ..., t_m\}$（词项集合）

目标：对每个文档计算相关性分数，返回最相关的文档。

### 1.2 词袋模型（Bag of Words）

将文档表示为词频向量，忽略词序：

$$\mathbf{d} = [tf(t_1, d), tf(t_2, d), ..., tf(t_V, d)]$$

其中 $tf(t, d)$ 是词项 $t$ 在文档 $d$ 中的出现次数，$V$ 是词汇表大小。

---

## 二、TF-IDF：BM25 的前身

### 2.1 词频（Term Frequency）

直觉：一个词在文档中出现越多次，该文档与这个词越相关。

$$TF(t, d) = f(t, d)$$

其中 $f(t, d)$ 是词 $t$ 在文档 $d$ 中的出现次数。

**问题**：原始词频没有上限，一个词出现 100 次不应该比出现 10 次重要 10 倍。

**改进**：对数词频

$$TF(t, d) = 1 + \log(f(t, d)) \quad \text{if } f(t, d) > 0$$

### 2.2 逆文档频率（Inverse Document Frequency）

直觉：一个词在越少的文档中出现，它的区分能力越强。

$$IDF(t) = \log \frac{N}{df(t)}$$

其中：
- $N$：文档总数
- $df(t)$：包含词 $t$ 的文档数

**例子**：
- "的"：几乎所有文档都有，$df \approx N$，$IDF \approx 0$
- "量子计算"：很少文档有，$df$ 很小，$IDF$ 很大

### 2.3 TF-IDF 公式

$$\text{TF-IDF}(t, d) = TF(t, d) \times IDF(t)$$

文档与查询的相关性：

$$\text{score}(q, d) = \sum_{t \in q} \text{TF-IDF}(t, d)$$

### 2.4 TF-IDF 的问题

1. **TF 没有饱和**：词频增长，分数线性增长
2. **没有文档长度归一化**：长文档天然词频高，占优势
3. **IDF 可能为负**：当 $df(t) > N/2$ 时

---

## 三、BM25 算法

### 3.1 完整公式

$$\text{BM25}(q, d) = \sum_{t \in q} IDF(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$

其中：
- $f(t, d)$：词 $t$ 在文档 $d$ 中的词频
- $|d|$：文档 $d$ 的长度（词数）
- $avgdl$：所有文档的平均长度
- $k_1$：词频饱和参数，通常 $k_1 \in [1.2, 2.0]$
- $b$：文档长度归一化参数，通常 $b = 0.75$

### 3.2 IDF 的改进形式

原始 IDF 可能为负，BM25 使用改进版本：

$$IDF(t) = \log \frac{N - df(t) + 0.5}{df(t) + 0.5}$$

或者更常用的形式（Lucene/Elasticsearch 使用）：

$$IDF(t) = \log \left(1 + \frac{N - df(t) + 0.5}{df(t) + 0.5}\right)$$

这保证 IDF 始终为正。

### 3.3 TF 饱和机制

BM25 的 TF 部分：

$$\text{TF}_{BM25}(t, d) = \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot B}$$

其中 $B = 1 - b + b \cdot \frac{|d|}{avgdl}$

**分析**：

当 $f(t, d) \to \infty$：

$$\text{TF}_{BM25} \to \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d)} = k_1 + 1$$

TF 有上限 $k_1 + 1$，实现了**饱和效应**。

**$k_1$ 的影响**：

| $k_1$ 值 | 效果 |
|----------|------|
| $k_1 = 0$ | TF 完全不考虑，只看是否出现 |
| $k_1$ 小 | 快速饱和，词频差异影响小 |
| $k_1$ 大 | 慢速饱和，词频差异影响大 |
| $k_1 \to \infty$ | 退化为线性 TF |

### 3.4 文档长度归一化

归一化因子：

$$B = 1 - b + b \cdot \frac{|d|}{avgdl}$$

**分析**：

- 当 $|d| = avgdl$：$B = 1$，无调整
- 当 $|d| > avgdl$：$B > 1$，降低长文档的分数
- 当 $|d| < avgdl$：$B < 1$，提高短文档的分数

**$b$ 的影响**：

| $b$ 值 | 效果 |
|--------|------|
| $b = 0$ | 不考虑文档长度 |
| $b = 1$ | 完全按长度比例归一化 |
| $b = 0.75$ | 经验最优值 |

### 3.5 公式推导的直觉

BM25 可以从概率模型推导（Robertson-Sparck Jones 模型），但直觉理解更重要：

1. **IDF**：稀有词更重要
2. **TF 饱和**：词频增加的边际效益递减
3. **长度归一化**：消除长文档的不公平优势

---

## 四、BM25 变体

### 4.1 BM25L

解决长文档惩罚过重的问题：

$$\text{TF}_{BM25L} = \frac{f(t, d) + \delta}{B + \delta} \cdot (k_1 + 1)$$

其中 $\delta$ 是平滑参数，通常 $\delta = 0.5$。

### 4.2 BM25+

确保 TF 的下界：

$$\text{BM25+}(q, d) = \sum_{t \in q} IDF(t) \cdot \left(\frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot B} + \delta\right)$$

即使 $f(t, d) = 0$，也有一个小的正分数。

### 4.3 BM25F（Field-aware）

考虑文档的不同字段（标题、正文、锚文本等）：

$$\text{BM25F}(q, d) = \sum_{t \in q} IDF(t) \cdot \frac{\tilde{f}(t, d) \cdot (k_1 + 1)}{\tilde{f}(t, d) + k_1}$$

其中：

$$\tilde{f}(t, d) = \sum_{field} w_{field} \cdot \frac{f(t, d, field)}{B_{field}}$$

不同字段有不同的权重 $w_{field}$ 和归一化因子 $B_{field}$。

---

## 五、数学性质分析

### 5.1 TF 饱和曲线

设 $B = 1$（平均长度文档），分析 TF 部分：

$$g(x) = \frac{x(k_1 + 1)}{x + k_1}$$

**导数**：

$$g'(x) = \frac{(k_1 + 1)(x + k_1) - x(k_1 + 1)}{(x + k_1)^2} = \frac{k_1(k_1 + 1)}{(x + k_1)^2}$$

$g'(x) > 0$：单调递增
$g''(x) < 0$：凹函数（边际效益递减）

**极限**：

$$\lim_{x \to 0} g(x) = 0$$
$$\lim_{x \to \infty} g(x) = k_1 + 1$$

### 5.2 与概率模型的关系

BM25 源自二元独立模型（Binary Independence Model）的概率推导。

假设：
- 文档相关性是二元的（相关/不相关）
- 词项之间独立

相关性的对数几率比：

$$\log \frac{P(R|d, q)}{P(\bar{R}|d, q)} = \sum_{t \in q} \log \frac{P(t|R)}{P(t|\bar{R})} \cdot \log \frac{P(\bar{t}|\bar{R})}{P(\bar{t}|R)}$$

通过一系列近似和参数化，可以得到 BM25 公式。

---

## 六、实现细节

### 6.1 Python 实现

```python
import math
from collections import Counter

class BM25:
    def __init__(self, documents, k1=1.5, b=0.75):
        """
        documents: list of list of tokens
        """
        self.k1 = k1
        self.b = b
        self.documents = documents
        self.N = len(documents)
        
        # 计算文档长度
        self.doc_lengths = [len(doc) for doc in documents]
        self.avgdl = sum(self.doc_lengths) / self.N
        
        # 计算词频
        self.doc_freqs = [Counter(doc) for doc in documents]
        
        # 计算 DF
        self.df = Counter()
        for doc in documents:
            for term in set(doc):
                self.df[term] += 1
        
        # 预计算 IDF
        self.idf = {}
        for term, df in self.df.items():
            self.idf[term] = math.log(
                (self.N - df + 0.5) / (df + 0.5) + 1
            )
    
    def score(self, query, doc_idx):
        """
        计算查询与文档的 BM25 分数
        """
        doc = self.documents[doc_idx]
        doc_len = self.doc_lengths[doc_idx]
        doc_freq = self.doc_freqs[doc_idx]
        
        score = 0.0
        for term in query:
            if term not in self.idf:
                continue
            
            tf = doc_freq.get(term, 0)
            idf = self.idf[term]
            
            # BM25 TF 部分
            B = 1 - self.b + self.b * (doc_len / self.avgdl)
            tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * B)
            
            score += idf * tf_component
        
        return score
    
    def search(self, query, top_k=10):
        """
        返回最相关的 top_k 个文档
        """
        scores = [(i, self.score(query, i)) for i in range(self.N)]
        scores.sort(key=lambda x: -x[1])
        return scores[:top_k]


# 使用示例
documents = [
    ["机器", "学习", "是", "人工智能", "的", "分支"],
    ["深度", "学习", "是", "机器", "学习", "的", "子集"],
    ["自然", "语言", "处理", "使用", "机器", "学习"],
    ["计算机", "视觉", "是", "人工智能", "应用"],
]

bm25 = BM25(documents)
query = ["机器", "学习"]
results = bm25.search(query, top_k=3)

for doc_idx, score in results:
    print(f"Doc {doc_idx}: {score:.4f} - {documents[doc_idx]}")
```

### 6.2 倒排索引优化

实际系统使用倒排索引加速：

```python
class BM25WithInvertedIndex:
    def __init__(self, documents, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.N = len(documents)
        
        # 文档长度
        self.doc_lengths = [len(doc) for doc in documents]
        self.avgdl = sum(self.doc_lengths) / self.N
        
        # 构建倒排索引
        # term -> [(doc_id, tf), ...]
        self.inverted_index = {}
        self.df = Counter()
        
        for doc_id, doc in enumerate(documents):
            term_freq = Counter(doc)
            for term, tf in term_freq.items():
                if term not in self.inverted_index:
                    self.inverted_index[term] = []
                self.inverted_index[term].append((doc_id, tf))
                self.df[term] += 1
        
        # 预计算 IDF
        self.idf = {
            term: math.log((self.N - df + 0.5) / (df + 0.5) + 1)
            for term, df in self.df.items()
        }
    
    def search(self, query, top_k=10):
        """
        使用倒排索引的高效搜索
        """
        scores = {}
        
        for term in query:
            if term not in self.inverted_index:
                continue
            
            idf = self.idf[term]
            
            # 只遍历包含该词的文档
            for doc_id, tf in self.inverted_index[term]:
                doc_len = self.doc_lengths[doc_id]
                B = 1 - self.b + self.b * (doc_len / self.avgdl)
                tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * B)
                
                if doc_id not in scores:
                    scores[doc_id] = 0.0
                scores[doc_id] += idf * tf_component
        
        # 排序返回
        results = sorted(scores.items(), key=lambda x: -x[1])
        return results[:top_k]
```

### 6.3 复杂度分析

**暴力搜索**：$O(N \cdot |q|)$，遍历所有文档

**倒排索引**：$O(|q| \cdot \bar{L})$，其中 $\bar{L}$ 是平均倒排列表长度

当查询词是稀有词时，$\bar{L} \ll N$，加速显著。

---

## 七、BM25 vs 向量检索

### 7.1 对比

| 特性 | BM25 | 向量检索 |
|------|------|----------|
| 匹配方式 | 精确词匹配 | 语义匹配 |
| 同义词处理 | ❌ 无法处理 | ✅ 自动处理 |
| 专有名词 | ✅ 精确匹配 | ⚠️ 可能匹配错误 |
| 计算速度 | 极快 | 较快 |
| 可解释性 | 高 | 低 |
| 冷启动 | ✅ 无需训练 | ❌ 需要 Embedding 模型 |

### 7.2 失败案例

**BM25 失败**：
```
Query: "苹果手机"
Doc: "iPhone 是最受欢迎的智能手机"
BM25: 无法匹配（没有共同词）
向量: 可以匹配（语义相似）
```

**向量检索失败**：
```
Query: "Python 3.9 新特性"
Doc1: "Python 3.9 引入了字典合并运算符"
Doc2: "Python 是一种流行的编程语言，版本众多"
向量: 可能认为 Doc2 更相关（更多语义重叠）
BM25: 正确识别 Doc1（精确匹配 "3.9"）
```

### 7.3 混合检索

结合两者优势：

```python
def hybrid_search(query, bm25_index, vector_index, alpha=0.5):
    """
    alpha: 向量检索的权重
    """
    # BM25 检索
    bm25_results = bm25_index.search(query, top_k=100)
    bm25_scores = {doc_id: score for doc_id, score in bm25_results}
    
    # 向量检索
    query_embedding = embed(query)
    vector_results = vector_index.search(query_embedding, top_k=100)
    vector_scores = {doc_id: score for doc_id, score in vector_results}
    
    # 分数归一化
    bm25_scores = normalize_scores(bm25_scores)
    vector_scores = normalize_scores(vector_scores)
    
    # 融合
    all_docs = set(bm25_scores.keys()) | set(vector_scores.keys())
    final_scores = {}
    for doc_id in all_docs:
        bm25_s = bm25_scores.get(doc_id, 0)
        vector_s = vector_scores.get(doc_id, 0)
        final_scores[doc_id] = (1 - alpha) * bm25_s + alpha * vector_s
    
    return sorted(final_scores.items(), key=lambda x: -x[1])
```

---

## 八、实践建议

### 8.1 参数调优

| 参数 | 默认值 | 调优建议 |
|------|--------|----------|
| $k_1$ | 1.5 | 短文档用较小值，长文档用较大值 |
| $b$ | 0.75 | 文档长度差异大时用较大值 |

### 8.2 预处理

1. **分词**：中文需要分词，英文需要 tokenization
2. **小写化**：统一大小写
3. **去停用词**：移除 "的"、"是"、"the"、"is" 等
4. **词干提取**：running → run（英文）

### 8.3 常用库

```python
# rank_bm25
from rank_bm25 import BM25Okapi

documents = [doc.split() for doc in corpus]
bm25 = BM25Okapi(documents)
scores = bm25.get_scores(query.split())

# Elasticsearch
from elasticsearch import Elasticsearch

es = Elasticsearch()
es.search(index="my_index", body={
    "query": {
        "match": {
            "content": "机器学习"
        }
    }
})
```

### 8.4 BM25 在 RAG 中的应用

1. **混合检索**：BM25 + 向量检索
2. **Hard Negative 挖掘**：用 BM25 找到词汇相似但语义不相关的文档
3. **关键词过滤**：先用 BM25 粗筛，再用向量精排
