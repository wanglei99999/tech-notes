# 向量相似度度量详解

## 一、距离与相似度的关系

**距离（Distance）**：衡量两个向量的差异程度，值越小越相似
**相似度（Similarity）**：衡量两个向量的相似程度，值越大越相似

两者通常可以互相转换：

$$\text{similarity} = f(\text{distance})$$

常见转换：
- $\text{sim} = -\text{dist}$
- $\text{sim} = \frac{1}{1 + \text{dist}}$
- $\text{sim} = e^{-\text{dist}}$

---

## 二、欧氏距离（Euclidean Distance / L2 Distance）

### 2.1 定义

$$d_{L2}(\mathbf{a}, \mathbf{b}) = ||\mathbf{a} - \mathbf{b}||_2 = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}$$

平方欧氏距离（计算更快，省去开方）：

$$d_{L2}^2(\mathbf{a}, \mathbf{b}) = ||\mathbf{a} - \mathbf{b}||_2^2 = \sum_{i=1}^{n} (a_i - b_i)^2$$

### 2.2 几何意义

欧氏距离是两点之间的直线距离，即勾股定理的推广。

二维空间：
$$d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$

### 2.3 性质

1. **非负性**：$d(\mathbf{a}, \mathbf{b}) \geq 0$
2. **同一性**：$d(\mathbf{a}, \mathbf{b}) = 0 \Leftrightarrow \mathbf{a} = \mathbf{b}$
3. **对称性**：$d(\mathbf{a}, \mathbf{b}) = d(\mathbf{b}, \mathbf{a})$
4. **三角不等式**：$d(\mathbf{a}, \mathbf{c}) \leq d(\mathbf{a}, \mathbf{b}) + d(\mathbf{b}, \mathbf{c})$

这四条性质使欧氏距离成为一个**度量（Metric）**。

### 2.4 展开形式

$$d_{L2}^2(\mathbf{a}, \mathbf{b}) = ||\mathbf{a}||^2 + ||\mathbf{b}||^2 - 2\mathbf{a}^\top\mathbf{b}$$

推导：
$$\begin{align}
||\mathbf{a} - \mathbf{b}||^2 &= (\mathbf{a} - \mathbf{b})^\top(\mathbf{a} - \mathbf{b}) \\
&= \mathbf{a}^\top\mathbf{a} - 2\mathbf{a}^\top\mathbf{b} + \mathbf{b}^\top\mathbf{b} \\
&= ||\mathbf{a}||^2 + ||\mathbf{b}||^2 - 2\mathbf{a}^\top\mathbf{b}
\end{align}$$

### 2.5 优缺点

**优点**：
- 直观，符合物理空间的距离概念
- 满足度量公理，可用于度量空间的算法

**缺点**：
- 对向量的模长敏感
- 高维空间中，距离趋于集中（维度诅咒）

---

## 三、余弦相似度（Cosine Similarity）

### 3.1 定义

$$\cos(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{||\mathbf{a}|| \cdot ||\mathbf{b}||} = \frac{\sum_{i=1}^{n} a_i b_i}{\sqrt{\sum_{i=1}^{n} a_i^2} \cdot \sqrt{\sum_{i=1}^{n} b_i^2}}$$

### 3.2 几何意义

余弦相似度等于两个向量夹角的余弦值。

```
        b
       /
      /  θ
     /______ a

cos(θ) = 余弦相似度
```

- $\cos(\theta) = 1$：夹角为 0°，方向完全相同
- $\cos(\theta) = 0$：夹角为 90°，正交
- $\cos(\theta) = -1$：夹角为 180°，方向完全相反

### 3.3 值域

$$\cos(\mathbf{a}, \mathbf{b}) \in [-1, 1]$$

对于非负向量（如词频向量）：

$$\cos(\mathbf{a}, \mathbf{b}) \in [0, 1]$$

### 3.4 余弦距离

将相似度转换为距离：

$$d_{cos}(\mathbf{a}, \mathbf{b}) = 1 - \cos(\mathbf{a}, \mathbf{b})$$

值域：$[0, 2]$

**注意**：余弦距离不满足三角不等式，不是严格的度量！

反例：
$$\mathbf{a} = [1, 0], \mathbf{b} = [1, 1], \mathbf{c} = [0, 1]$$
$$d_{cos}(\mathbf{a}, \mathbf{c}) = 1$$
$$d_{cos}(\mathbf{a}, \mathbf{b}) + d_{cos}(\mathbf{b}, \mathbf{c}) = (1 - \frac{1}{\sqrt{2}}) + (1 - \frac{1}{\sqrt{2}}) \approx 0.586$$
$$1 > 0.586 \text{，违反三角不等式}$$

### 3.5 优缺点

**优点**：
- 只关注方向，不受向量长度影响
- 对文本相似度特别有效（文档长度不影响相似度）

**缺点**：
- 不是严格的度量
- 无法区分向量的"强度"

---

## 四、点积（Dot Product / Inner Product）

### 4.1 定义

$$\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^\top\mathbf{b} = \sum_{i=1}^{n} a_i b_i$$

### 4.2 几何意义

$$\mathbf{a} \cdot \mathbf{b} = ||\mathbf{a}|| \cdot ||\mathbf{b}|| \cdot \cos(\theta)$$

点积 = 向量长度的乘积 × 夹角余弦

也可以理解为：$\mathbf{a}$ 在 $\mathbf{b}$ 方向上的投影长度 × $||\mathbf{b}||$

### 4.3 值域

$$\mathbf{a} \cdot \mathbf{b} \in (-\infty, +\infty)$$

### 4.4 与余弦相似度的关系

当向量已归一化（$||\mathbf{a}|| = ||\mathbf{b}|| = 1$）时：

$$\mathbf{a} \cdot \mathbf{b} = \cos(\mathbf{a}, \mathbf{b})$$

**这就是为什么大多数 Embedding 模型输出归一化向量**——可以用更快的点积代替余弦相似度。

### 4.5 优缺点

**优点**：
- 计算最快（只需要乘法和加法）
- 归一化后等价于余弦相似度

**缺点**：
- 对向量长度敏感
- 值域无界，不便于比较

---

## 五、三者的数学关系

### 5.1 归一化向量的情况

设 $\hat{\mathbf{a}} = \frac{\mathbf{a}}{||\mathbf{a}||}$，$\hat{\mathbf{b}} = \frac{\mathbf{b}}{||\mathbf{b}||}$

则：
$$\hat{\mathbf{a}} \cdot \hat{\mathbf{b}} = \cos(\mathbf{a}, \mathbf{b})$$

$$||\hat{\mathbf{a}} - \hat{\mathbf{b}}||^2 = 2 - 2\cos(\mathbf{a}, \mathbf{b}) = 2(1 - \hat{\mathbf{a}} \cdot \hat{\mathbf{b}})$$

因此：
$$d_{L2}^2 = 2 \cdot d_{cos} = 2(1 - \text{dot product})$$

**结论**：对于归一化向量，三种度量是等价的，只是线性变换关系。

### 5.2 转换公式

| 已知 | 求 | 公式（归一化向量） |
|------|----|--------------------|
| 点积 | 余弦相似度 | $\cos = \text{dot}$ |
| 点积 | L2 距离 | $d_{L2} = \sqrt{2(1-\text{dot})}$ |
| 余弦相似度 | L2 距离 | $d_{L2} = \sqrt{2(1-\cos)}$ |
| L2 距离 | 余弦相似度 | $\cos = 1 - \frac{d_{L2}^2}{2}$ |

---

## 六、其他距离度量

### 6.1 曼哈顿距离（Manhattan Distance / L1 Distance）

$$d_{L1}(\mathbf{a}, \mathbf{b}) = ||\mathbf{a} - \mathbf{b}||_1 = \sum_{i=1}^{n} |a_i - b_i|$$

几何意义：在网格上只能沿坐标轴移动的最短路径。

### 6.2 闵可夫斯基距离（Minkowski Distance）

$$d_p(\mathbf{a}, \mathbf{b}) = ||\mathbf{a} - \mathbf{b}||_p = \left(\sum_{i=1}^{n} |a_i - b_i|^p\right)^{1/p}$$

- $p=1$：曼哈顿距离
- $p=2$：欧氏距离
- $p \to \infty$：切比雪夫距离 $d_\infty = \max_i |a_i - b_i|$

### 6.3 马氏距离（Mahalanobis Distance）

$$d_M(\mathbf{a}, \mathbf{b}) = \sqrt{(\mathbf{a} - \mathbf{b})^\top \Sigma^{-1} (\mathbf{a} - \mathbf{b})}$$

其中 $\Sigma$ 是协方差矩阵。

特点：考虑了特征之间的相关性和尺度差异。

### 6.4 汉明距离（Hamming Distance）

用于二值向量：

$$d_H(\mathbf{a}, \mathbf{b}) = \sum_{i=1}^{n} \mathbb{1}[a_i \neq b_i]$$

即不同位的数量。

### 6.5 杰卡德相似度（Jaccard Similarity）

用于集合：

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

用于二值向量：

$$J(\mathbf{a}, \mathbf{b}) = \frac{\sum_i \min(a_i, b_i)}{\sum_i \max(a_i, b_i)}$$

---

## 七、高维空间的特殊性质

### 7.1 维度诅咒（Curse of Dimensionality）

在高维空间中，距离度量的区分能力下降。

**现象**：随着维度增加，最近邻和最远邻的距离趋于相同。

$$\lim_{d \to \infty} \frac{d_{max} - d_{min}}{d_{min}} \to 0$$

**直觉解释**：高维空间中，所有点都趋向于分布在超球面的表面，彼此距离相近。

### 7.2 数学证明

设 $n$ 个点均匀分布在 $d$ 维单位超立方体 $[0,1]^d$ 中。

对于查询点 $\mathbf{q}$，到最近邻的期望距离：

$$\mathbb{E}[d_{min}] \approx \frac{1}{2} \left(\frac{1}{n}\right)^{1/d} \cdot \sqrt{d}$$

当 $d \to \infty$：
$$\mathbb{E}[d_{min}] \to \frac{\sqrt{d}}{2}$$

所有点到查询点的距离都趋向于 $\frac{\sqrt{d}}{2}$。

### 7.3 对 ANN 的影响

- 高维空间中，ANN 算法的加速效果下降
- 需要更多的候选点才能保证召回率
- 降维（如 PCA）可以缓解，但可能损失信息

---

## 八、实践中的选择

### 8.1 选择指南

| 场景 | 推荐度量 | 理由 |
|------|----------|------|
| 文本相似度 | 余弦相似度 | 不受文档长度影响 |
| 归一化向量 | 点积 | 计算最快，等价于余弦 |
| 图像特征 | 欧氏距离 | 保留强度信息 |
| 推荐系统 | 点积 | 可以学习用户/物品的"强度" |
| 二值特征 | 汉明距离 | 高效，适合 LSH |

### 8.2 归一化的重要性

```python
import numpy as np

def normalize(v):
    norm = np.linalg.norm(v)
    return v / norm if norm > 0 else v

# 归一化后
a_norm = normalize(a)
b_norm = normalize(b)

# 三种度量等价
dot_product = np.dot(a_norm, b_norm)
cosine_sim = dot_product  # 相等
l2_dist = np.sqrt(2 * (1 - dot_product))
```

### 8.3 向量数据库的支持

| 数据库 | 支持的度量 |
|--------|------------|
| FAISS | L2, Inner Product |
| Milvus | L2, IP, Cosine, Jaccard, Hamming |
| Pinecone | Cosine, Euclidean, Dot Product |
| Chroma | L2, IP, Cosine |

### 8.4 性能对比

```python
import numpy as np
import time

d = 768
n = 100000
a = np.random.random((n, d)).astype('float32')
b = np.random.random(d).astype('float32')

# 点积（最快）
start = time.time()
dots = a @ b
print(f"Dot product: {time.time() - start:.4f}s")

# 余弦相似度
start = time.time()
norms_a = np.linalg.norm(a, axis=1)
norm_b = np.linalg.norm(b)
cosines = (a @ b) / (norms_a * norm_b)
print(f"Cosine: {time.time() - start:.4f}s")

# L2 距离
start = time.time()
l2_dists = np.linalg.norm(a - b, axis=1)
print(f"L2: {time.time() - start:.4f}s")
```

典型结果：
- 点积：0.01s
- 余弦：0.02s
- L2：0.05s

**结论**：如果向量已归一化，优先使用点积。
