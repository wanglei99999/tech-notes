# RAG 核心知识详解

## 一、Embedding 原理

### 什么是 Embedding？

Embedding 是将离散的文本（词、句子、文档）映射到连续的高维向量空间的技术。核心思想是：**语义相似的文本，在向量空间中距离更近**。

### 从 Word2Vec 说起

Word2Vec（2013）是 Embedding 的开山之作，核心假设是**分布式假设**：
> 一个词的含义由它周围的词决定

两种训练方式：
- **CBOW**：用上下文预测中心词
- **Skip-gram**：用中心词预测上下文

```
句子: "The cat sits on the mat"
Skip-gram 训练样本:
  (cat, The), (cat, sits)
  (sits, cat), (sits, on)
  ...
```

训练后，每个词得到一个固定维度的向量（如 300 维），神奇的是：
```
vec("King") - vec("Man") + vec("Woman") ≈ vec("Queen")
```

### 句子级 Embedding

Word2Vec 只能得到词向量，句子怎么办？

**早期方案**：词向量平均/加权平均 → 效果一般，丢失语序信息

**现代方案**：基于 Transformer 的预训练模型
- **BERT**：双向编码器，取 [CLS] token 或平均池化
- **Sentence-BERT**：专门为句子相似度优化的 BERT 变体
- **BGE、M3E、text-embedding-ada**：当前主流的 Embedding 模型

### Embedding 模型的训练

核心是**对比学习**（后面详细讲），基本思路：
- 正样本对：语义相似的句子（如问答对、同义句）
- 负样本对：语义不相关的句子
- 训练目标：拉近正样本距离，推远负样本距离

### 向量维度的意义

每一维并没有明确的语义解释，但整体上：
- 高维空间有更强的表达能力
- 常见维度：384、768、1024、1536
- 维度越高，存储和计算成本越大

---

## 二、ANN（近似最近邻）算法

### 为什么需要 ANN？

精确最近邻搜索需要遍历所有向量，时间复杂度 O(n)。当向量库达到百万、千万级别时，暴力搜索不可接受。

ANN 用**少量精度损失**换取**数量级的速度提升**。

### 主流 ANN 算法

#### 1. HNSW（Hierarchical Navigable Small World）

**核心思想**：构建多层图结构，高层稀疏用于快速定位，底层稠密用于精确搜索。

```
Layer 2:  A -------- D          (稀疏，跳跃式导航)
          |          |
Layer 1:  A --- B -- D --- E    (中等密度)
          |    |     |     |
Layer 0:  A-B-C-D-E-F-G-H-I-J   (稠密，所有节点)
```

搜索过程：
1. 从最高层的入口点开始
2. 在当前层贪心搜索，找到最近的节点
3. 下降到下一层，继续搜索
4. 在最底层返回 top-k 结果

**优点**：查询速度快，召回率高
**缺点**：内存占用大，构建索引慢

#### 2. IVF（Inverted File Index）

**核心思想**：先聚类，查询时只搜索最近的几个聚类。

```
训练阶段：
  所有向量 → K-Means 聚类 → 得到 n 个聚类中心（centroids）
  每个向量分配到最近的聚类

查询阶段：
  Query → 找到最近的 nprobe 个聚类中心 → 只在这些聚类内搜索
```

**参数**：
- `nlist`：聚类数量，通常 sqrt(n) 到 4*sqrt(n)
- `nprobe`：查询时搜索的聚类数，越大越准但越慢

#### 3. PQ（Product Quantization）

**核心思想**：向量压缩，用更少的空间存储向量。

```
原始向量 (128维) → 分成 8 段 (每段16维)
每段独立做 K-Means (K=256) → 每段用 1 字节表示聚类 ID
最终：128*4 = 512 字节 → 8 字节 (64倍压缩)
```

**优点**：极大减少内存占用
**缺点**：有精度损失

#### 组合使用：IVF-PQ、HNSW-PQ

实际系统常组合使用：
- **IVF-PQ**：先 IVF 缩小范围，再用 PQ 压缩存储
- **HNSW-PQ**：HNSW 图结构 + PQ 压缩

### 选型建议

| 场景 | 推荐算法 | 理由 |
|------|----------|------|
| 数据量小（<100万） | HNSW | 召回率高，速度快 |
| 数据量大，内存有限 | IVF-PQ | 内存占用小 |
| 追求极致召回率 | HNSW | 精度最高 |
| 需要频繁更新 | HNSW | 支持增量插入 |

---

## 三、向量相似度度量

### 1. 余弦相似度（Cosine Similarity）

$$\text{cosine}(A, B) = \frac{A \cdot B}{||A|| \times ||B||}$$

- 范围：[-1, 1]，1 表示完全相同方向
- **只关注方向，不关注长度**
- 最常用于文本相似度

### 2. 欧氏距离（Euclidean Distance / L2）

$$\text{L2}(A, B) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}$$

- 范围：[0, +∞)，0 表示完全相同
- **同时考虑方向和长度**
- 对向量的模长敏感

### 3. 点积（Dot Product / Inner Product）

$$\text{dot}(A, B) = \sum_{i=1}^{n} A_i \times B_i$$

- 范围：(-∞, +∞)
- **当向量已归一化时，等价于余弦相似度**
- 计算最快

### 如何选择？

```python
# 如果向量已归一化（模长为1）
# 三者关系：
cosine(A, B) = dot(A, B)  # 完全相等
L2(A, B) = sqrt(2 - 2 * dot(A, B))  # 可互相转换
```

**实践建议**：
- 大多数 Embedding 模型输出已归一化 → 用**点积**（最快）
- 不确定是否归一化 → 用**余弦相似度**（最稳）
- 需要考虑向量"强度" → 用**欧氏距离**

---

## 四、BM25 算法

### TF-IDF 回顾

BM25 是 TF-IDF 的改进版，先理解 TF-IDF：

- **TF（Term Frequency）**：词在文档中出现的频率
- **IDF（Inverse Document Frequency）**：词的稀有程度

$$\text{TF-IDF}(t, d) = TF(t, d) \times IDF(t)$$

问题：TF 没有上限，一个词出现 100 次不应该比出现 10 次重要 10 倍。

### BM25 公式

$$\text{BM25}(q, d) = \sum_{t \in q} IDF(t) \times \frac{TF(t, d) \times (k_1 + 1)}{TF(t, d) + k_1 \times (1 - b + b \times \frac{|d|}{avgdl})}$$

关键改进：
1. **TF 饱和**：通过 k1 参数控制，TF 增长到一定程度后趋于饱和
2. **文档长度归一化**：通过 b 参数控制，避免长文档天然占优

### 参数含义

- **k1**（通常 1.2-2.0）：控制 TF 饱和速度，越大饱和越慢
- **b**（通常 0.75）：控制文档长度影响，0 表示不考虑长度，1 表示完全归一化

### BM25 vs 向量检索

| 特性 | BM25 | 向量检索 |
|------|------|----------|
| 匹配方式 | 精确词匹配 | 语义匹配 |
| "苹果手机" vs "iPhone" | ❌ 匹配不上 | ✅ 能匹配 |
| "银行" vs "银行" | ✅ 精确匹配 | 可能匹配到"河岸" |
| 计算速度 | 极快（倒排索引） | 较快（ANN） |
| 可解释性 | 高 | 低 |

**结论**：两者互补，混合检索效果最好。

---

## 五、倒排索引

### 正排索引 vs 倒排索引

```
正排索引（Forward Index）：
  Doc1 → ["苹果", "手机", "发布"]
  Doc2 → ["苹果", "水果", "营养"]

倒排索引（Inverted Index）：
  "苹果" → [Doc1, Doc2]
  "手机" → [Doc1]
  "水果" → [Doc2]
  "发布" → [Doc1]
  "营养" → [Doc2]
```

### 倒排索引结构

```
词项 → (文档ID, 词频, 位置列表)

"苹果" → [(Doc1, 2, [0, 15]), (Doc2, 1, [0])]
         文档1出现2次，位置0和15；文档2出现1次，位置0
```

### 查询过程

```
Query: "苹果 手机"

1. 查倒排表：
   "苹果" → [Doc1, Doc2]
   "手机" → [Doc1]

2. 求交集（AND）或并集（OR）：
   AND: [Doc1]
   OR:  [Doc1, Doc2]

3. 计算相关性分数（BM25）并排序
```

### 为什么快？

- 时间复杂度：O(查询词数 × 平均倒排列表长度)
- 不需要遍历所有文档
- 可以用跳表（Skip List）加速列表求交

---

## 六、Bi-encoder vs Cross-encoder

这是理解 RAG 召回和重排的关键！

### Bi-encoder（双塔模型）

```
Query: "什么是机器学习"     Document: "机器学习是AI的分支..."
        ↓                           ↓
   [Encoder]                   [Encoder]
        ↓                           ↓
   Query向量                    Doc向量
        ↓___________↓___________↓
              相似度计算
              (点积/余弦)
```

**特点**：
- Query 和 Document **独立编码**
- Document 向量可以**离线预计算**并存储
- 查询时只需计算 Query 向量，然后做向量检索
- **速度快，适合召回阶段**

### Cross-encoder（交叉编码器）

```
Query: "什么是机器学习"  +  Document: "机器学习是AI的分支..."
                    ↓
            [SEP] 拼接
                    ↓
    "[CLS] 什么是机器学习 [SEP] 机器学习是AI的分支... [SEP]"
                    ↓
              [Encoder]
                    ↓
            相关性分数 (0-1)
```

**特点**：
- Query 和 Document **联合编码**，充分交互
- 每个 Query-Document 对都要**实时计算**
- 不能预计算，无法用于大规模检索
- **精度高，适合重排阶段**

### 为什么 Cross-encoder 更准？

Bi-encoder 的问题：Query 和 Document 独立编码，无法捕捉细粒度交互。

```
Query: "苹果公司市值"
Doc1: "苹果公司市值突破3万亿" → 高度相关
Doc2: "苹果富含维生素，市值..." → 不太相关

Bi-encoder: 两个文档都包含"苹果"和"市值"，向量可能很接近
Cross-encoder: 能理解 Doc1 的"苹果公司"是整体，Doc2 是两个不相关的词
```

### 实际应用

```
召回阶段（Bi-encoder）：
  100万文档 → ANN检索 → Top 100 候选
  耗时：~10ms

重排阶段（Cross-encoder）：
  100 候选 → 逐个打分 → Top 10 结果
  耗时：~100ms（100次前向传播）
```

---

## 七、对比学习（Contrastive Learning）

### 核心思想

通过构造正负样本对，让模型学会区分相似和不相似的样本。

```
正样本对：(Query, 相关Document) → 拉近距离
负样本对：(Query, 不相关Document) → 推远距离
```

### InfoNCE Loss

最常用的对比学习损失函数：

$$\mathcal{L} = -\log \frac{\exp(sim(q, d^+)/\tau)}{\exp(sim(q, d^+)/\tau) + \sum_{d^-}\exp(sim(q, d^-)/\tau)}$$

- $q$：Query 向量
- $d^+$：正样本 Document 向量
- $d^-$：负样本 Document 向量
- $\tau$：温度参数，控制分布的平滑程度

**直觉理解**：这是一个 softmax 分类问题，让正样本的概率最大化。

### 负样本的重要性

负样本质量直接影响模型效果：

1. **In-batch Negatives**：同一 batch 内其他样本作为负样本
   - 简单高效
   - 负样本可能太简单

2. **Hard Negatives**：精心挑选的难负样本
   - BM25 召回的非相关文档
   - 其他 Query 的正样本
   - 效果更好，但构造成本高

### 训练数据来源

- 搜索日志：(Query, 点击文档) 作为正样本
- 问答对：(问题, 答案)
- 自然语言推理数据集：蕴含关系作为正样本
- 人工标注

---

## 八、Chunking 策略

### 为什么要分片？

1. **LLM 上下文长度限制**：不能把整本书塞进去
2. **检索粒度**：太大则噪声多，太小则信息不完整
3. **Embedding 质量**：过长文本的 Embedding 质量下降

### 常见分片策略

#### 1. 固定长度分片

```python
def fixed_length_chunk(text, chunk_size=512, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i:i + chunk_size])
    return chunks
```

**优点**：简单
**缺点**：可能从句子中间切断

#### 2. 递归分割（LangChain 默认）

```python
分隔符优先级：["\n\n", "\n", " ", ""]

1. 先尝试按 "\n\n"（段落）分割
2. 如果块太大，按 "\n"（行）分割
3. 如果还太大，按 " "（词）分割
4. 最后按字符分割
```

**优点**：尽量保持语义完整性
**缺点**：块大小不均匀

#### 3. 语义分割

```python
1. 将文本分成句子
2. 计算相邻句子的 Embedding 相似度
3. 在相似度低的地方切分（话题转换点）
```

**优点**：语义最完整
**缺点**：计算成本高

### Chunk Size 的影响

| Chunk Size | 优点 | 缺点 |
|------------|------|------|
| 小（100-200） | 检索精确 | 上下文不足，需要更多 chunks |
| 中（500-1000） | 平衡 | 通用选择 |
| 大（1000-2000） | 上下文丰富 | 可能引入噪声 |

### Overlap 的作用

```
无 Overlap：
  [Chunk1: 句子1,2,3] [Chunk2: 句子4,5,6]
  问题：如果答案跨越句子3和4，两个chunk都不完整

有 Overlap：
  [Chunk1: 句子1,2,3,4] [Chunk2: 句子3,4,5,6]
  句子3,4 在两个 chunk 中都有，提高召回率
```

**经验值**：Overlap 通常设为 Chunk Size 的 10-20%

---

## 九、Query 改写与扩展

### 为什么需要 Query 改写？

用户的原始 Query 往往：
- 太短，信息不足
- 口语化，与文档风格不匹配
- 有歧义

### 常见技术

#### 1. Query 扩展

```
原始 Query: "LLM 是什么"
扩展后: "LLM 是什么 大语言模型 Large Language Model 定义 介绍"
```

方法：
- 同义词扩展
- LLM 生成相关词
- 基于知识图谱扩展

#### 2. HyDE（Hypothetical Document Embeddings）

核心思想：让 LLM 先生成一个"假想答案"，用假想答案去检索。

```
Query: "量子计算的优势是什么"
        ↓
      [LLM]
        ↓
假想答案: "量子计算相比经典计算具有指数级加速优势，
         特别是在因数分解、优化问题、量子模拟等领域..."
        ↓
    [Embedding]
        ↓
    向量检索
```

**为什么有效**：假想答案的风格更接近文档，比短 Query 的 Embedding 质量更高。

#### 3. Query Decomposition

将复杂问题拆解为多个子问题：

```
Query: "比较 Python 和 Java 在机器学习领域的优劣"
        ↓
子问题1: "Python 在机器学习领域的优势"
子问题2: "Python 在机器学习领域的劣势"
子问题3: "Java 在机器学习领域的优势"
子问题4: "Java 在机器学习领域的劣势"
        ↓
分别检索，合并结果
```

#### 4. Step-back Prompting

先问一个更抽象的问题，获取背景知识：

```
Query: "如果温度升高2度，冰川会怎样"
        ↓
Step-back: "气候变化对冰川的影响机制是什么"
        ↓
先检索背景知识，再回答具体问题
```

---

## 十、混合检索与融合策略

### 为什么要混合检索？

| 检索方式 | 擅长 | 不擅长 |
|----------|------|--------|
| 向量检索 | 语义匹配、同义词 | 精确关键词、专有名词 |
| BM25 | 精确匹配、关键词 | 语义理解、同义词 |

**混合检索 = 向量检索 + BM25，取长补短**

### RRF（Reciprocal Rank Fusion）

最常用的结果融合算法：

$$RRF(d) = \sum_{r \in R} \frac{1}{k + r(d)}$$

- $R$：所有排序结果列表
- $r(d)$：文档 d 在某个列表中的排名
- $k$：常数，通常取 60

```python
def rrf_fusion(rankings, k=60):
    scores = {}
    for ranking in rankings:
        for rank, doc in enumerate(ranking, 1):
            if doc not in scores:
                scores[doc] = 0
            scores[doc] += 1 / (k + rank)
    return sorted(scores.items(), key=lambda x: -x[1])

# 示例
bm25_results = ["doc1", "doc3", "doc2", "doc5"]
vector_results = ["doc2", "doc1", "doc4", "doc3"]

# doc1: 1/(60+1) + 1/(60+2) = 0.0164 + 0.0161 = 0.0325
# doc2: 1/(60+3) + 1/(60+1) = 0.0159 + 0.0164 = 0.0323
# doc3: 1/(60+2) + 1/(60+4) = 0.0161 + 0.0156 = 0.0317
```

### 其他融合策略

1. **分数归一化后加权**
   ```
   final_score = α * norm(bm25_score) + (1-α) * norm(vector_score)
   ```

2. **Convex Combination**
   ```
   需要将不同来源的分数映射到同一尺度
   ```

3. **学习排序（Learning to Rank）**
   ```
   训练一个模型来学习最优的融合权重
   ```

### 实践建议

- 先尝试 RRF，简单有效
- 如果有标注数据，可以训练融合权重
- 向量检索和 BM25 的权重通常在 0.5:0.5 到 0.7:0.3 之间

---

## 总结：RAG 优化方向

```
┌─────────────────────────────────────────────────────────┐
│                    RAG 优化全景图                        │
├─────────────────────────────────────────────────────────┤
│  索引阶段                                                │
│  ├── Chunking 策略优化                                   │
│  ├── Embedding 模型选择/微调                             │
│  └── 混合索引（向量 + 倒排）                              │
├─────────────────────────────────────────────────────────┤
│  检索阶段                                                │
│  ├── Query 改写/扩展                                     │
│  ├── 混合检索（向量 + BM25）                              │
│  ├── 重排（Cross-encoder）                               │
│  └── 结果融合策略                                        │
├─────────────────────────────────────────────────────────┤
│  生成阶段                                                │
│  ├── Prompt 工程                                         │
│  ├── 上下文压缩                                          │
│  └── 引用溯源                                            │
└─────────────────────────────────────────────────────────┘
```
