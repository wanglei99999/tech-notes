# 倒排索引详解

## 一、索引的基本概念

### 1.1 为什么需要索引？

假设有 100 万篇文档，每篇 1000 词，查询 "机器学习"：

**暴力搜索**：遍历每篇文档的每个词
- 时间复杂度：$O(N \times L) = O(10^9)$
- 不可接受

**使用索引**：直接定位包含查询词的文档
- 时间复杂度：$O(|posting\_list|)$
- 通常远小于 N

### 1.2 正排索引 vs 倒排索引

**正排索引（Forward Index）**：文档 → 词项

```
Doc1 → ["机器", "学习", "算法", "深度"]
Doc2 → ["自然", "语言", "处理", "机器"]
Doc3 → ["计算机", "视觉", "深度", "学习"]
```

用途：已知文档 ID，获取文档内容

**倒排索引（Inverted Index）**：词项 → 文档

```
"机器" → [Doc1, Doc2]
"学习" → [Doc1, Doc3]
"深度" → [Doc1, Doc3]
"自然" → [Doc2]
"语言" → [Doc2]
"处理" → [Doc2]
"计算机" → [Doc3]
"视觉" → [Doc3]
```

用途：已知词项，快速找到包含它的文档

---

## 二、倒排索引的结构

### 2.1 基本结构

倒排索引由两部分组成：

1. **词典（Dictionary/Lexicon）**：所有唯一词项的集合
2. **倒排列表（Posting List）**：每个词项对应的文档列表

```
词典                    倒排列表
┌─────────┐            ┌─────────────────────┐
│ 机器    │ ────────→  │ [1, 2]              │
├─────────┤            ├─────────────────────┤
│ 学习    │ ────────→  │ [1, 3]              │
├─────────┤            ├─────────────────────┤
│ 深度    │ ────────→  │ [1, 3]              │
├─────────┤            ├─────────────────────┤
│ 自然    │ ────────→  │ [2]                 │
└─────────┘            └─────────────────────┘
```

### 2.2 倒排记录（Posting）

每个倒排记录可以包含更多信息：

```
基本形式：
  term → [doc_id1, doc_id2, ...]

带词频：
  term → [(doc_id1, tf1), (doc_id2, tf2), ...]

带位置：
  term → [(doc_id1, tf1, [pos1, pos2, ...]), ...]
```

**完整示例**：

```
"机器" → [
    (Doc1, tf=2, positions=[0, 15]),
    (Doc2, tf=1, positions=[5])
]
```

表示：
- Doc1 中 "机器" 出现 2 次，位置是第 0 和第 15 个词
- Doc2 中 "机器" 出现 1 次，位置是第 5 个词

### 2.3 词典的数据结构

#### 哈希表（Hash Table）

```python
dictionary = {
    "机器": posting_list_1,
    "学习": posting_list_2,
    ...
}
```

- 查找：$O(1)$
- 不支持前缀查询、范围查询
- 内存占用较大

#### 字典树（Trie）

```
        root
       / | \
      机 学 深
      |  |  |
      器 习 度
```

- 查找：$O(|term|)$
- 支持前缀查询
- 内存占用大（每个字符一个节点）

#### 有限状态自动机（FST）

Lucene/Elasticsearch 使用的结构，结合了 Trie 的前缀共享和后缀共享。

```
     ┌─ a ─ t ─ (cat)
root ─┤
     └─ c ─ a ─ r ─ (car)
              └─ t ─ (cart)
```

- 查找：$O(|term|)$
- 支持前缀查询
- 内存高效（共享前缀和后缀）

---

## 三、倒排索引的构建

### 3.1 基本构建流程

```
文档集合 → 分词 → 排序 → 合并 → 倒排索引
```

### 3.2 单遍扫描算法（SPIMI）

**Single-Pass In-Memory Indexing**

```python
def spimi_invert(documents, memory_limit):
    """
    单遍扫描构建倒排索引
    """
    dictionary = {}  # term -> posting_list
    
    for doc_id, doc in enumerate(documents):
        for term in tokenize(doc):
            if term not in dictionary:
                dictionary[term] = []
            dictionary[term].append(doc_id)
            
            # 内存满了，写入磁盘
            if memory_usage() > memory_limit:
                write_block_to_disk(dictionary)
                dictionary = {}
    
    # 写入最后一个块
    if dictionary:
        write_block_to_disk(dictionary)
    
    # 合并所有块
    merge_blocks()
```

### 3.3 外部排序合并

当数据量超过内存时，需要分块构建再合并：

```python
def merge_blocks(block_files):
    """
    多路归并排序
    """
    # 打开所有块文件
    readers = [open(f) for f in block_files]
    
    # 优先队列，按词项排序
    heap = []
    for i, reader in enumerate(readers):
        term, posting = read_next(reader)
        if term:
            heappush(heap, (term, posting, i))
    
    # 合并
    output = open("final_index", "w")
    current_term = None
    current_postings = []
    
    while heap:
        term, posting, reader_idx = heappop(heap)
        
        if term != current_term:
            if current_term:
                write_posting(output, current_term, current_postings)
            current_term = term
            current_postings = []
        
        current_postings.extend(posting)
        
        # 读取下一个
        next_term, next_posting = read_next(readers[reader_idx])
        if next_term:
            heappush(heap, (next_term, next_posting, reader_idx))
    
    # 写入最后一个词项
    if current_term:
        write_posting(output, current_term, current_postings)
```

### 3.4 复杂度分析

设：
- $N$：文档数
- $T$：总词项数（所有文档的词数之和）
- $M$：内存大小

**时间复杂度**：
- 分词：$O(T)$
- 排序：$O(T \log T)$
- 合并：$O(T \log(T/M))$

**空间复杂度**：$O(T)$（最终索引大小）

---

## 四、倒排列表的压缩

### 4.1 为什么需要压缩？

倒排列表可能非常长：
- 常见词 "的" 可能出现在 90% 的文档中
- 存储 100 万个 32 位整数需要 4MB

### 4.2 差值编码（Delta Encoding）

倒排列表是有序的，存储差值而非原值：

```
原始：[1, 5, 10, 15, 100, 105]
差值：[1, 4, 5, 5, 85, 5]
```

差值通常较小，可以用更少的位数表示。

### 4.3 可变字节编码（Variable Byte Encoding）

用可变长度的字节表示整数：

```
编码规则：
- 每个字节的最高位是延续位（1=继续，0=结束）
- 低 7 位存储数据

示例：
5 → 10000101 (1字节)
130 → 00000001 10000010 (2字节)
    = (1 << 7) + 2 = 130
```

```python
def vbyte_encode(n):
    """可变字节编码"""
    bytes_list = []
    while True:
        bytes_list.insert(0, n % 128)
        if n < 128:
            break
        n //= 128
    bytes_list[-1] += 128  # 设置结束标志
    return bytes(bytes_list)

def vbyte_decode(data):
    """可变字节解码"""
    n = 0
    for byte in data:
        if byte < 128:
            n = 128 * n + byte
        else:
            n = 128 * n + (byte - 128)
            return n
    return n
```

### 4.4 Gamma 编码

更激进的压缩，适合小整数：

```
编码规则：
1. 计算 floor(log2(n)) = k
2. 用 k 个 1 后跟一个 0 表示长度
3. 用 k 位表示 n - 2^k

示例：
5 = 4 + 1 = 2^2 + 1
k = 2
编码：110 01 (5位)

13 = 8 + 5 = 2^3 + 5
k = 3
编码：1110 101 (7位)
```

### 4.5 PForDelta

现代搜索引擎常用的编码：

1. 将倒排列表分成固定大小的块（如 128 个数）
2. 找到能表示大多数数的最小位宽 $b$
3. 用 $b$ 位表示大多数数，异常值单独存储

```
块：[1, 2, 3, 100, 2, 1, 3, 2, ...]
大多数数 < 8，用 3 位表示
异常值 100 单独存储
```

### 4.6 压缩效果对比

| 编码方式 | 压缩比 | 解码速度 |
|----------|--------|----------|
| 无压缩 | 1x | 最快 |
| 可变字节 | 2-4x | 快 |
| Gamma | 3-5x | 中等 |
| PForDelta | 3-4x | 快（SIMD 优化） |

---

## 五、查询处理

### 5.1 布尔查询

#### AND 查询

```
Query: "机器" AND "学习"

"机器" → [1, 2, 5, 8, 10]
"学习" → [1, 3, 5, 7, 10]

结果：[1, 5, 10]（交集）
```

**朴素算法**：

```python
def intersect(list1, list2):
    """两个有序列表求交集"""
    result = []
    i, j = 0, 0
    while i < len(list1) and j < len(list2):
        if list1[i] == list2[j]:
            result.append(list1[i])
            i += 1
            j += 1
        elif list1[i] < list2[j]:
            i += 1
        else:
            j += 1
    return result
```

时间复杂度：$O(|L_1| + |L_2|)$

#### OR 查询

```
Query: "机器" OR "学习"

"机器" → [1, 2, 5, 8, 10]
"学习" → [1, 3, 5, 7, 10]

结果：[1, 2, 3, 5, 7, 8, 10]（并集）
```

```python
def union(list1, list2):
    """两个有序列表求并集"""
    result = []
    i, j = 0, 0
    while i < len(list1) and j < len(list2):
        if list1[i] == list2[j]:
            result.append(list1[i])
            i += 1
            j += 1
        elif list1[i] < list2[j]:
            result.append(list1[i])
            i += 1
        else:
            result.append(list2[j])
            j += 1
    result.extend(list1[i:])
    result.extend(list2[j:])
    return result
```

#### NOT 查询

```
Query: "机器" AND NOT "深度"

"机器" → [1, 2, 5, 8, 10]
"深度" → [1, 5]

结果：[2, 8, 10]（差集）
```

### 5.2 跳表优化（Skip List）

对于长倒排列表，可以添加跳表加速交集运算：

```
原始列表：[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]

添加跳表（步长=3）：
Level 1: 1 ─────→ 7 ─────→ 13 ─────→ 19
Level 0: 1 → 3 → 5 → 7 → 9 → 11 → 13 → 15 → 17 → 19
```

**带跳表的交集算法**：

```python
def intersect_with_skip(list1, list2):
    """使用跳表加速的交集"""
    result = []
    i, j = 0, 0
    
    while i < len(list1) and j < len(list2):
        if list1[i] == list2[j]:
            result.append(list1[i])
            i += 1
            j += 1
        elif list1[i] < list2[j]:
            # 尝试跳跃
            if list1.has_skip(i) and list1.skip_to(i) <= list2[j]:
                while list1.has_skip(i) and list1.skip_to(i) <= list2[j]:
                    i = list1.skip_index(i)
            else:
                i += 1
        else:
            # 对称处理 list2
            if list2.has_skip(j) and list2.skip_to(j) <= list1[i]:
                while list2.has_skip(j) and list2.skip_to(j) <= list1[i]:
                    j = list2.skip_index(j)
            else:
                j += 1
    
    return result
```

**最优跳表步长**：$\sqrt{L}$，其中 $L$ 是列表长度

### 5.3 查询优化

#### 按列表长度排序

先处理短列表，减少中间结果大小：

```
Query: "的" AND "机器" AND "学习"

"的" → 100万文档
"机器" → 1万文档
"学习" → 5千文档

优化顺序：先 "学习" ∩ "机器"，再 ∩ "的"
```

#### 提前终止

对于 top-k 查询，可以提前终止：

```python
def top_k_search(query_terms, k):
    """
    使用 WAND 算法的简化版本
    """
    # 按 IDF 排序词项
    terms = sorted(query_terms, key=lambda t: idf[t], reverse=True)
    
    # 计算每个词项的最大贡献
    max_scores = {t: idf[t] * max_tf_component for t in terms}
    
    # 当前 top-k 的最小分数
    threshold = 0
    results = []
    
    # ... WAND 算法实现
```

---

## 六、索引更新

### 6.1 静态索引 vs 动态索引

**静态索引**：
- 构建后不再修改
- 更新需要重建整个索引
- 适合数据不常变化的场景

**动态索引**：
- 支持增量更新
- 结构更复杂
- 适合实时搜索场景

### 6.2 增量索引策略

#### 辅助索引（Auxiliary Index）

```
主索引（大，不常更新）
    ↓
辅助索引（小，频繁更新）
    ↓
定期合并
```

```python
class IncrementalIndex:
    def __init__(self):
        self.main_index = {}  # 主索引
        self.aux_index = {}   # 辅助索引
        self.deleted = set()  # 删除标记
    
    def add_document(self, doc_id, terms):
        """添加文档到辅助索引"""
        for term in terms:
            if term not in self.aux_index:
                self.aux_index[term] = []
            self.aux_index[term].append(doc_id)
    
    def delete_document(self, doc_id):
        """标记删除"""
        self.deleted.add(doc_id)
    
    def search(self, term):
        """搜索时合并两个索引"""
        results = []
        
        # 从主索引获取
        if term in self.main_index:
            results.extend(self.main_index[term])
        
        # 从辅助索引获取
        if term in self.aux_index:
            results.extend(self.aux_index[term])
        
        # 过滤已删除的
        results = [d for d in results if d not in self.deleted]
        
        return results
    
    def merge(self):
        """合并辅助索引到主索引"""
        for term, postings in self.aux_index.items():
            if term not in self.main_index:
                self.main_index[term] = []
            self.main_index[term].extend(postings)
            self.main_index[term].sort()
        
        # 清理已删除的文档
        for term in self.main_index:
            self.main_index[term] = [
                d for d in self.main_index[term] 
                if d not in self.deleted
            ]
        
        self.aux_index = {}
        self.deleted = set()
```

#### 对数合并（Logarithmic Merge）

维护多个不同大小的索引层级：

```
Level 0: 最小，最新（内存中）
Level 1: 2x 大小
Level 2: 4x 大小
...
Level k: 2^k x 大小
```

当某层满了，与下一层合并。

**优点**：
- 写入开销分摊：$O(\log N)$ 次合并
- 查询需要搜索 $O(\log N)$ 个索引

---

## 七、分布式倒排索引

### 7.1 分片策略

#### 按文档分片（Document Partitioning）

```
Shard 1: Doc 1-1000 的完整索引
Shard 2: Doc 1001-2000 的完整索引
...
```

查询时：
1. 广播查询到所有分片
2. 每个分片返回本地 top-k
3. 合并结果

#### 按词项分片（Term Partitioning）

```
Shard 1: 词项 A-M 的倒排列表
Shard 2: 词项 N-Z 的倒排列表
```

查询时：
1. 根据查询词路由到对应分片
2. 获取倒排列表
3. 在协调节点计算分数

### 7.2 Elasticsearch 的实现

```
Index
├── Shard 0
│   ├── Segment 0 (不可变)
│   ├── Segment 1 (不可变)
│   └── Segment 2 (不可变)
├── Shard 1
│   └── ...
└── Shard 2
    └── ...
```

- **Index**：逻辑上的索引
- **Shard**：物理分片，可分布在不同节点
- **Segment**：Lucene 的不可变索引单元

---

## 八、实践建议

### 8.1 索引设计

1. **选择合适的分词器**
   - 中文：jieba、HanLP
   - 英文：标准分词 + 词干提取

2. **字段设计**
   - 需要搜索的字段建立倒排索引
   - 需要排序/聚合的字段使用 doc_values
   - 不需要搜索的字段禁用索引

3. **分片数量**
   - 单个分片建议 10-50GB
   - 分片数 = 数据量 / 单分片大小

### 8.2 查询优化

1. **使用 filter 而非 query**
   - filter 可以缓存，不计算分数

2. **避免通配符开头**
   - `*abc` 需要扫描整个词典

3. **使用 routing**
   - 相关文档路由到同一分片，减少跨分片查询

### 8.3 监控指标

- 索引大小
- 段数量（过多需要合并）
- 查询延迟
- 缓存命中率
