# 第1章 NLP基础概念

## 1.1 什么是NLP

**NLP（Natural Language Processing，自然语言处理）** = 让计算机理解、处理人类语言的技术

NLP结合了多个学科：
- 计算机科学（怎么实现）
- 语言学（语言规则）
- 人工智能（让机器学习）

## 1.2 NLP发展历程

| 时期 | 方法 | 特点 |
|------|------|------|
| 1940s-1960s | 规则方法 | 人工写规则 |
| 1970s-1990s | 统计方法 | 用概率统计 |
| 2000s至今 | 深度学习 | 神经网络自动学习 |

**关键节点：**
- 1950年：图灵测试
- 2013年：Word2Vec（词向量革命）
- 2018年：BERT（预训练模型时代）
- 2022年：ChatGPT（大模型时代）

## 1.3 常见NLP任务

### 1.3.1 中文分词
```
输入：今天天气真好
输出：今天 | 天气 | 真 | 好
```

### 1.3.2 词性标注（POS Tagging）
```
She/代词  is/动词  playing/动词  the/限定词  guitar/名词
```

### 1.3.3 命名实体识别（NER）
```
输入：李雷和韩梅梅计划去上海旅行
输出：李雷[人名]、韩梅梅[人名]、上海[地名]
```

### 1.3.4 文本分类
```
"NBA季后赛将于下周开始" → 体育
"苹果发布新款Macbook" → 科技
```

### 1.3.5 机器翻译
```
今天天气很好 → The weather is very nice today
```

### 1.3.6 自动问答（QA）
输入问题，输出答案。ChatGPT就是这个任务的终极形态。

## 1.4 文本表示与词向量

计算机只认数字，必须把文字转成向量。

### 词向量基础概念（重要！）

#### 什么是词向量？

**词向量（Word Embedding）** = 用一组数字（向量）来表示一个词

```
"苹果" → [0.2, -0.5, 0.8, 0.1, -0.3, ...]  # 一串数字
```

#### 为什么叫"向量"？

**向量**是数学概念，简单理解就是"一组有序的数字"：
```
二维向量：[3, 4]           → 平面上的一个点
三维向量：[1, 2, 3]        → 空间中的一个点
N维向量：[x1, x2, ..., xN] → N维空间中的一个点
```

词向量通常是 100-300 维，虽然无法想象 300 维空间，但数学上可以计算。

#### 向量有什么用？

**1. 计算相似度**

**余弦相似度**（Cosine Similarity）：

$\text{cos}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \times ||\mathbf{B}||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \times \sqrt{\sum_{i=1}^{n} B_i^2}}$

- 范围：$[-1, 1]$
- 1 表示完全相同方向
- 0 表示正交（无关）
- -1 表示完全相反

**欧氏距离**（Euclidean Distance）：

$d(\mathbf{A}, \mathbf{B}) = ||\mathbf{A} - \mathbf{B}||_2 = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}$

**2. 向量运算表达语义**
```
向量(国王) - 向量(男人) + 向量(女人) ≈ 向量(王后)
```

### 1.4.1 One-Hot 编码（最原始）

```python
# 词汇表：[苹果, 香蕉, 椅子]
苹果 = [1, 0, 0]
香蕉 = [0, 1, 0]  
椅子 = [0, 0, 1]
```

**问题**：
- 维度爆炸（词汇表多大，向量就多长）
- 稀疏（99%都是0）
- 无语义（苹果和香蕉向量完全不相关，点积为0）

### 1.4.2 N-gram语言模型（详解）

**核心问题**：给定前面的词，预测下一个词的概率是多少？

**马尔可夫假设**：一个词出现的概率只取决于它前面的 N-1 个词（简化计算）。

#### 不同的 N 值

| 名称 | N值 | 含义 | 例子 |
|------|-----|------|------|
| Unigram | 1 | 只看当前词本身 | P(好) |
| Bigram | 2 | 看前1个词 | P(好 \| 天气) |
| Trigram | 3 | 看前2个词 | P(好 \| 天气, 很) |

#### 计算示例

计算句子 **"今天天气很好"** 的概率：

**Bigram（N=2）**：
```
P(今天天气很好) = P(今天|<开始>) × P(天气|今天) × P(很|天气) × P(好|很)
```

**概率怎么算？从语料库统计词频：**
```
P(好 | 天气很) = count("天气很好") / count("天气很")

例：语料中"天气很好"出现1000次，"天气很"出现1200次
P(好 | 天气很) = 1000 / 1200 = 0.833
```

#### N-gram 的问题

1. **数据稀疏**：N越大，组合越多，很多组合没出现过，概率为0
2. **长距离依赖捕捉不了**：Trigram最多看前2个词，远处的关联看不到
3. **存储开销大**：需要存储所有N-gram的统计结果

#### 平滑技术（解决稀疏）

**加一平滑（Laplace Smoothing）**：给每个组合都加1，避免概率为0

$P(w | context) = \frac{count(context, w) + 1}{count(context) + V}$

其中 $V$ 是词汇表大小。

**Kneser-Ney 平滑**（更先进）：

$P_{KN}(w_i | w_{i-1}) = \frac{\max(c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})} + \lambda(w_{i-1}) P_{continuation}(w_i)$

其中 $d$ 是折扣参数，$\lambda$ 是归一化常数。
### 1.4.3 Word2Vec（2013，革命性！详解）

Word2Vec 是 Google 在 2013 年提出的，是现代 NLP 的奠基石。

#### 为什么 Word2Vec 是革命性的？

One-Hot 的三大问题，Word2Vec 全部解决：

| 问题 | One-Hot | Word2Vec |
|------|---------|----------|
| 维度 | 词汇表大小（5万维） | 固定低维（100-300维） |
| 稀疏性 | 99.99%是0 | 稠密向量，没有0 |
| 语义 | 无（猫和狗完全正交） | 有（猫和狗向量相近） |

#### 核心思想：用上下文定义词义

**语言学假设**：一个词的含义由它周围的词决定。

> "You shall know a word by the company it keeps." — J.R. Firth, 1957

```
"我养了一只___，它会抓老鼠"  → 猫
"我养了一只___，它会看家"    → 狗
```

"猫"和"狗"经常出现在相似的上下文中，所以它们的向量应该相近。

#### 两种训练方式（详细对比）

用例子说明：**句子 "我 喜欢 在 早上 吃 苹果 和 香蕉"，中心词是"吃"，窗口大小=2**

```
句子：我 喜欢 在 早上 吃 苹果 和 香蕉
                    ↑
                  中心词
          [在,早上]   [苹果,和]
           左窗口      右窗口
```

**1. CBOW（Continuous Bag of Words）—— 多个输入 → 一个输出**

任务：用上下文词预测中心词

```
输入：[在, 早上, 苹果, 和]  （4个词）
输出：吃                    （1个词）

训练样本：([在, 早上, 苹果, 和], 吃)  ← 只有1个样本
```

计算过程：
```
步骤1：把4个上下文词转成向量
       在    → [0.1, 0.2, 0.3]
       早上  → [0.2, 0.1, 0.4]
       苹果  → [0.3, 0.5, 0.1]
       和    → [0.1, 0.3, 0.2]

步骤2：求平均得到一个向量
       平均 → [0.175, 0.275, 0.25]

步骤3：通过输出层预测
       [0.175, 0.275, 0.25] → Softmax → P(吃) 应该最大
```

图示：
```
    在 ──┐
  早上 ──┼──→ 平均 ──→ 神经网络 ──→ 预测"吃"
  苹果 ──┤
    和 ──┘
```

**2. Skip-gram —— 一个输入 → 多个输出**

任务：用中心词预测上下文词

```
输入：吃                    （1个词）
输出：[在, 早上, 苹果, 和]  （4个词，分别预测）

训练样本：
  (吃, 在)      ← 样本1
  (吃, 早上)    ← 样本2
  (吃, 苹果)    ← 样本3
  (吃, 和)      ← 样本4
```

计算过程：
```
步骤1：把中心词"吃"转成向量
       吃 → [0.4, 0.2, 0.5]

步骤2：用这个向量分别预测每个上下文词
       [0.4, 0.2, 0.5] → Softmax → P(在) 应该高
       [0.4, 0.2, 0.5] → Softmax → P(早上) 应该高
       [0.4, 0.2, 0.5] → Softmax → P(苹果) 应该高
       [0.4, 0.2, 0.5] → Softmax → P(和) 应该高
```

图示：
```
                    ┌──→ 预测"在"
                    ├──→ 预测"早上"
    吃 ──→ 神经网络 ─┼──→ 预测"苹果"
                    └──→ 预测"和"
```

**3. 关键区别对比**

| 对比项 | CBOW | Skip-gram |
|--------|------|-----------|
| 输入 | 多个上下文词 | 1个中心词 |
| 输出 | 1个中心词 | 多个上下文词 |
| 训练样本数 | 1个 | 窗口内词数个（本例4个） |
| 训练速度 | 快 | 慢 |
| 低频词效果 | 一般 | 更好 |
| 适用场景 | 小数据集 | 大数据集 |

**为什么 Skip-gram 对低频词更好？**

因为同样的语料，Skip-gram 产生的训练样本更多。低频词作为中心词时，也能产生多个训练样本，有更多机会被学习。

**实际中 Skip-gram 用得更多**

#### 网络结构

```
输入层          隐藏层           输出层
(One-Hot)      (词向量)         (Softmax)

  V维            N维              V维
[0,0,1,0,...] → [0.2,-0.5,...] → [0.01,0.02,0.8,...]
                    ↑
              这就是我们要的词向量！
```

- V = 词汇表大小（比如 5 万）
- N = 词向量维度（通常 100-300）
- **训练完成后，隐藏层的权重矩阵就是所有词的词向量！**

**Word2Vec 的目标函数**：

Skip-gram 的目标是最大化以下对数似然：

$\mathcal{L} = \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$

其中 $c$ 是窗口大小，$T$ 是语料库大小。

条件概率使用 Softmax 计算：

$P(w_O | w_I) = \frac{\exp(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I})}{\sum_{w=1}^{V} \exp(\mathbf{v}'_w \cdot \mathbf{v}_{w_I})}$

其中：
- $\mathbf{v}_w$ 是词 $w$ 的输入向量（Input Embedding）
- $\mathbf{v}'_w$ 是词 $w$ 的输出向量（Output Embedding）

#### 窗口大小

训练时需要定义"上下文"的范围，通常窗口大小取 5（左右各5个词）：

```
句子：我 喜欢 在 早上 吃 苹果 和 香蕉
                    ↑
                  中心词
      [喜欢,在,早上]     [苹果,和,香蕉]
         左窗口              右窗口
```

#### 负采样（Negative Sampling）—— 加速训练（详解）

**问题：为什么需要负采样？**

Word2Vec 输出层用 Softmax 计算每个词的概率：
```
P(词i | 中心词) = exp(score_i) / Σ exp(score_j)
                                  ↑
                            要对所有词求和！
```

词汇表有 5 万个词，每次训练都要计算 5 万个词的分数，**太慢了！**

```
计算量对比：
原始 Softmax：每次训练计算 5万次
负采样：每次训练计算 1+K 次（K通常=5~20）
提速：约 2500 倍！
```

**核心思想：把多分类变成二分类**

```
原来的任务（5万分类）：
  输入：吃
  输出：在5万个词中，"苹果"的概率最高

负采样的任务（二分类）：
  输入：(吃, 苹果)
  输出：是真实的一对(1) 还是随机配的(0)
```

**具体步骤：**

步骤1：构造正样本（从真实语料提取）
```
句子："我 喜欢 吃 苹果"，中心词是"吃"

正样本：
  (吃, 喜欢) → 标签 1
  (吃, 苹果) → 标签 1
```

步骤2：构造负样本（随机采样）
```
负样本（随机从词汇表选词）：
  (吃, 汽车) → 标签 0
  (吃, 政治) → 标签 0
  (吃, 月亮) → 标签 0
```

步骤3：训练二分类器
```
输入 (吃, 苹果) → 模型 → 输出接近 1 ✓
输入 (吃, 汽车) → 模型 → 输出接近 0 ✓
```

**负样本怎么采样？**

不是完全随机，而是按词频的 0.75 次方采样：

$P(w_i) = \frac{f(w_i)^{0.75}}{\sum_{j=1}^{V} f(w_j)^{0.75}}$

其中 $f(w_i)$ 是词 $w_i$ 的频率。

为什么用 0.75 次方？压制高频词，给低频词更多机会：
```
词频：  的=1000, 苹果=100, 榴莲=10
原始：  的=91%, 苹果=9%, 榴莲=0.9%
0.75方：的=56%, 苹果=18%, 榴莲=5.6%  ← 更均衡
```

**负采样的目标函数**：

$\mathcal{L} = \log \sigma(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}'_{w_i} \cdot \mathbf{v}_{w_I})]$

其中：
- $\sigma(x) = \frac{1}{1 + e^{-x}}$ 是 Sigmoid 函数
- $k$ 是负样本数量（通常 5-20）
- $P_n(w)$ 是负采样分布

**训练效果：**
```
训练前：吃 ●    苹果 ●    汽车 ●  （随机分布）

训练后：吃 ● ←靠近→ ● 苹果
        
        汽车 ●  （远离"吃"）
```

负采样让正样本对的向量靠近，负样本对的向量远离。

#### 神奇的向量运算（Word2Vec一个很好的特性）

训练好的词向量可以做**语义运算**：

```
向量(国王) - 向量(男人) + 向量(女人) ≈ 向量(王后)
向量(巴黎) - 向量(法国) + 向量(日本) ≈ 向量(东京)
向量(更好) - 向量(好) + 向量(坏) ≈ 向量(更坏)
```

这说明 Word2Vec 学到了：
- 性别关系：男人→女人 ≈ 国王→王后
- 首都关系：法国→巴黎 ≈ 日本→东京
- 比较级关系：好→更好 ≈ 坏→更坏

#### 词向量可视化

把 300 维向量用 t-SNE 降到 2 维，可以看到语义相近的词自动聚在一起：

```
        动物区域          水果区域          国家区域
    猫 ● ● 狗         苹果 ● ● 香蕉      中国 ● ● 日本
      ● 老虎              ● 橙子              ● 韩国
```

#### Word2Vec 的局限

1. **静态向量**：一个词只有一个向量，无法处理一词多义
   ```
   "苹果很好吃" 和 "苹果发布新手机" 中的"苹果"向量相同！
   ```

2. **只用局部上下文**：窗口大小有限，捕捉不了长距离依赖

3. **无法处理未登录词（OOV）**：训练时没见过的词没有向量

这些问题后来被 ELMo、BERT 等模型解决了。

### 1.4.4 ELMo（2018，动态词向量，详解）

ELMo = Embeddings from Language Models

#### 要解决的问题：一词多义

Word2Vec 的致命缺陷：
```
"我买了一个苹果"     → 苹果 = 水果 🍎
"苹果发布了新手机"   → 苹果 = 公司 

但在 Word2Vec 中，两个"苹果"的向量完全相同！
```

#### 核心思想：根据上下文动态生成词向量

同一个词，在不同句子中，向量不同：
```
"我买了一个苹果"     → 苹果 = [0.8, 0.1, 0.2, ...]  # 偏向水果
"苹果发布了新手机"   → 苹果 = [0.1, 0.9, 0.3, ...]  # 偏向公司
```

#### 模型结构：双向 LSTM

```
输入句子：我 买了 一个 苹果

前向 LSTM（从左到右）：
我 → 买了 → 一个 → 苹果
                    ↓
              前向隐藏状态 h→

后向 LSTM（从右到左）：
我 ← 买了 ← 一个 ← 苹果
↓
后向隐藏状态 h←

最终词向量 = 组合(原始词向量, h→, h←)
```

**为什么要双向？**
- 前向：知道"苹果"前面是"一个"，可能是水果
- 后向：知道"苹果"后面是句号，确认是水果
- 双向结合：更全面理解上下文

#### 三层表示

ELMo 有多层 LSTM，每层捕捉不同信息：
```
第0层：原始词向量 → 词的基本语义
第1层：第一层LSTM → 句法信息（词性、语法）
第2层：第二层LSTM → 语义信息（词义、上下文）

最终词向量 = 三层的加权组合
```

**ELMo 的数学表示**：

对于输入 token $t_k$，ELMo 表示为：

$\text{ELMo}_k^{task} = E(R_k; \Theta^{task}) = \gamma^{task} \sum_{j=0}^{L} s_j^{task} \mathbf{h}_{k,j}^{LM}$

其中：
- $\mathbf{h}_{k,j}^{LM}$ 是第 $j$ 层的隐藏状态
- $s_j^{task}$ 是 Softmax 归一化的层权重（任务特定）
- $\gamma^{task}$ 是缩放因子
- $L$ 是 LSTM 层数

不同任务学习不同权重：
- 词性标注：更依赖第1层（句法）
- 情感分析：更依赖第2层（语义）

#### 训练方式：语言模型

**前向语言模型**：根据前文预测下一个词
```
输入：我 买了 一个
预测：苹果
```

**后向语言模型**：根据后文预测前一个词
```
输入：买了 一个 苹果
预测：我
```

#### 开创性贡献：预训练 + 微调范式

```
阶段1：预训练（Pre-training）
       在大量无标注文本上训练语言模型
       学习通用的语言理解能力
       
阶段2：微调（Fine-tuning）
       在具体任务上使用 ELMo 词向量
       针对任务调整权重
```

```
预训练（一次）           微调（每个任务）
    ↓                       ↓
大量文本 → ELMo模型 → 情感分析 → 情感模型
                    → 命名实体 → NER模型
                    → 问答任务 → QA模型
```

**好处**：预训练只做一次，可用于各种下游任务！

#### ELMo vs Word2Vec 对比

| 特性 | Word2Vec | ELMo |
|------|----------|------|
| 词向量类型 | 静态（固定） | 动态（随上下文变化） |
| 一词多义 | ❌ 无法处理 | ✅ 可以处理 |
| 上下文范围 | 局部窗口 | 整个句子 |
| 模型结构 | 简单神经网络 | 双向 LSTM |
| 训练任务 | 预测上下文词 | 语言模型 |

#### ELMo 的局限

1. **基于 LSTM**：序列计算，无法并行，训练慢
2. **拼接式双向**：前向后向只是拼接，不是真正交互
3. **特征提取器**：只提供词向量，下游任务还需单独模型

这些问题后来被 **BERT** 解决：
- Transformer 替代 LSTM（可并行）
- Masked LM 实现真正双向
- 整个模型端到端微调

#### ELMo 的历史地位

```
Word2Vec (2013)     ELMo (2018)      BERT (2018)      GPT-3 (2020)
    ↓                   ↓                ↓                ↓
静态词向量 ──→ 动态词向量 ──→ 预训练模型 ──→ 大语言模型
             开创预训练范式    Transformer     规模化
```

ELMo 是连接传统词向量和现代预训练模型的**桥梁**。

### 1.4.5 Token 与子词切分（现代 LLM 的基础）

#### 什么是 Token？

**Token** = 文本被切分后的最小单位

简单说，就是把一句话"切"成一块一块的，每一块就是一个 token。

```
文本 → 切分 → [token1, token2, token3, ...]
```

**Token 不一定等于"词"**，取决于切分方式：
```
词级别：  "I love playing" → ["I", "love", "playing"]  # 3个token
字符级别："I love" → ["I", " ", "l", "o", "v", "e"]    # 6个token
子词级别："playing" → ["play", "##ing"]               # 2个token
```

#### Token 和"词"的区别

| 概念 | 说明 | 例子 |
|------|------|------|
| 词（Word） | 语言学上的词 | "苹果"、"喜欢" |
| Token | 模型处理的单位 | 可能是词、子词、字符 |

```
GPT 的切分示例：
"ChatGPT is amazing" → ["Chat", "G", "PT", " is", " amazing"]
5 个 token，但只有 3 个词！
```

#### Tokenization（分词）

把文本切成 token 的过程叫 **Tokenization**，执行这个操作的工具叫 **Tokenizer（分词器）**。

```
文本 → Tokenizer → Token序列 → Token IDs → Embedding → 输入模型

"我喜欢苹果"
    ↓ Tokenizer
["我", "喜欢", "苹果"]
    ↓ 查词汇表
[152, 3847, 9823]      ← Token IDs
    ↓ 查 Embedding 矩阵
[[0.2, -0.1, ...],     ← 词向量
 [0.5, 0.3, ...],
 [0.1, 0.8, ...]]
    ↓
输入神经网络
```

#### 为什么需要子词切分？

**词级别的问题**：
1. 词汇表爆炸（英语几十万词）
2. 未登录词（OOV）无法处理
3. 形态变化：play/plays/played/playing 是4个不同的词

**字符级别的问题**：
1. 序列太长，计算量大
2. 单个字符没有语义

**子词切分的优势**：
```
"unhappiness" → ["un", "happi", "ness"]

- "un" = 否定前缀
- "happi" = happy 的词根  
- "ness" = 名词后缀

既不太长，又保留了语义！
```

#### 主流子词切分算法

**1. BPE（Byte Pair Encoding）—— GPT 系列使用**

从字符开始，不断合并最频繁出现的相邻对：
```
初始：所有单个字符 [a, b, c, ...]
第1轮：合并最频繁的对 "lo" → 新token "lo"
第2轮：合并 "low" → 新token "low"
...重复直到词汇表达到指定大小
```

**2. WordPiece —— BERT 使用**

类似 BPE，子词前加 `##` 表示是词的一部分：
```
"playing" → ["play", "##ing"]
"unhappy" → ["un", "##happy"]
```

**3. SentencePiece —— LLaMA/ChatGLM 使用**

用 `▁` 表示空格，语言无关：
```
"I love you" → ["▁I", "▁love", "▁you"]
```

**各模型使用的分词方法**：

| 模型 | 分词方法 | 词汇表大小 |
|------|----------|-----------|
| GPT-2/3/4 | BPE | 50,257 |
| BERT | WordPiece | 30,522 |
| LLaMA | SentencePiece | 32,000 |
| ChatGLM | SentencePiece | 130,344 |

#### 特殊 Token

| Token | 含义 | 用途 |
|-------|------|------|
| [PAD] | Padding | 填充短句子 |
| [UNK] | Unknown | 未知词 |
| [CLS] | Classification | BERT句子开头 |
| [SEP] | Separator | 分隔句子 |
| [MASK] | Mask | BERT训练遮盖词 |
| \<BOS\> | Begin | 句子开始 |
| \<EOS\> | End | 句子结束 |

#### 为什么 Token 数量重要？

**1. 计算成本**
- Token 越多，计算越慢，显存占用越大
- Transformer 的计算复杂度是 O(n²)，n 是 token 数

**2. 上下文长度限制**
```
模型能"记住"的内容有限：
- GPT-3.5: 4K / 16K token
- GPT-4: 8K / 32K / 128K token
- Claude: 100K / 200K token

你的输入 + 模型输出 总共不能超过限制
超过了就会"忘记"前面的内容
```

**3. API 计费**
```
OpenAI 按 token 收费：
GPT-4: $0.03 / 1K input tokens
       $0.06 / 1K output tokens

一篇 1000 字的中文文章 ≈ 500-800 token
```

**4. 语言模型的核心**

Token 是语言模型预测的基本单位：
```
语言模型任务：给定前面的 token，预测下一个 token

输入：[今天, 天气, 很]
预测：好（概率最高）

这就是 ChatGPT 生成文本的原理！
一个 token 一个 token 地预测出来
```

### 1.4.6 文本预处理流程（实际工程必备）

#### 完整流程

```
原始文本
    ↓
1. 文本清洗（去除噪声）
    ↓
2. 分词 Tokenization
    ↓
3. 转换为 Token IDs
    ↓
4. 添加特殊 Token
    ↓
5. Padding/Truncation
    ↓
6. 生成 Attention Mask
    ↓
7. 转换为 Embedding
    ↓
输入模型
```

#### 详细步骤示例

**步骤1：文本清洗**
```
原始："  Hello, World!!!   "
清洗后："Hello, World!"
```

**步骤2：分词**
```
"I love playing" → ["I", "love", "play", "##ing"]
```

**步骤3：转换为 Token IDs**
```
查词汇表：
{"[PAD]":0, "[CLS]":101, "[SEP]":102, "I":1045, "love":2293, ...}

["I", "love", "play", "##ing"] → [1045, 2293, 2377, 2075]
```

**步骤4：添加特殊 Token**
```
BERT格式：[CLS] I love play ##ing [SEP]
IDs：     [101, 1045, 2293, 2377, 2075, 102]
```

**步骤5：Padding 和 Truncation**
```
假设 max_length = 10

句子1: [101, 1045, 2293, 102]           # 长度4
句子2: [101, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 102]  # 长度12，超了

Padding后：
句子1: [101, 1045, 2293, 102, 0, 0, 0, 0, 0, 0]  # 用0填充

Truncation后：
句子2: [101, 1, 2, 3, 4, 5, 6, 7, 8, 102]  # 截断到10
```

**步骤6：生成 Attention Mask**
```
句子1: [101, 1045, 2293, 102, 0, 0, 0, 0, 0, 0]
mask:  [1,   1,    1,    1,   0, 0, 0, 0, 0, 0]
       真实token=1，padding=0
```

**步骤7：转换为 Embedding**
```
Token IDs → 查 Embedding 矩阵 → 词向量矩阵
[101, 1045, 2293, ...] → [[0.1,0.2,...], [0.3,0.1,...], ...]
形状：(seq_len, hidden_size) 如 (10, 768)
```

#### 代码示例（Hugging Face Transformers）

```python
from transformers import AutoTokenizer

# 加载分词器
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# 原始文本
texts = ["我喜欢吃苹果", "今天天气真好"]

# 一步完成所有预处理！
encoded = tokenizer(
    texts,
    padding=True,          # 自动padding
    truncation=True,       # 自动截断
    max_length=10,         # 最大长度
    return_tensors="pt"    # 返回PyTorch张量
)

print(encoded.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])

print(encoded['input_ids'])
# tensor([[ 101, 2769, 1599, 3614, 1391, 5765, 3362,  102,    0,    0],
#         [ 101,  791, 1921, 1921, 3698, 4696, 1962, 1557,  102,    0]])

print(encoded['attention_mask'])
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])
```

## 本章小结

### 核心概念速查表

| 概念 | 一句话解释 |
|------|-----------|
| NLP | 让计算机理解人类语言 |
| Token | 文本切分后的最小单位，模型处理的基本单元 |
| Tokenizer | 把文本切成 token 的工具 |
| 词向量/Embedding | 用一组数字（向量）表示一个词 |
| One-Hot | 最原始的词表示，稀疏、无语义 |
| Word2Vec | 低维稠密向量，能捕捉语义关系 |
| ELMo | 动态词向量，解决一词多义 |
| 子词切分 | 把词拆成更小的单位（BPE/WordPiece） |
| 语言模型 | 预测下一个 token 的概率模型 |

### 词向量发展脉络

```
One-Hot → Word2Vec → ELMo → BERT/GPT
(稀疏)    (静态稠密)  (动态)   (预训练大模型)
  ↓          ↓         ↓          ↓
无语义    有语义    一词多义   强大的语言理解
```

### 文本处理流程

```
原始文本 → 分词(Tokenization) → Token IDs → Embedding → 模型
```

### 关键技术对比

| 技术 | 年份 | 核心贡献 |
|------|------|---------|
| Word2Vec | 2013 | 词向量、语义运算 |
| ELMo | 2018 | 动态词向量、预训练范式 |
| BERT | 2018 | Transformer、双向编码 |
| GPT | 2018-now | 自回归生成、规模化 |

### 下一章预告

第2章将学习 **Transformer 架构**，这是现代 LLM 的核心：
- 注意力机制（Attention）
- 自注意力（Self-Attention）
- 多头注意力（Multi-Head Attention）
- Encoder-Decoder 结构

---
学习日期：2025年12月
