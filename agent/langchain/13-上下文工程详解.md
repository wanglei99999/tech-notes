# 上下文工程详解

构建 Agent（或任何 LLM 应用）的难点在于让它们足够可靠。原型可能工作正常，但在实际场景中经常失败。

## 为什么 Agent 会失败？

Agent 失败通常是因为内部的 LLM 调用采取了错误的行动。LLM 失败的原因有两个：

1. 底层 LLM 能力不足
2. 没有传递"正确"的上下文给 LLM

**大多数情况下，是第二个原因导致 Agent 不可靠。**

## 什么是上下文工程？

**上下文工程（Context Engineering）** = 以正确的格式提供正确的信息和工具，让 LLM 能够完成任务。

这是 AI 工程师的首要工作。缺乏"正确"的上下文是 Agent 不可靠的头号障碍。

## 你能控制什么？

要构建可靠的 Agent，需要控制 Agent 循环的每个步骤，以及步骤之间发生的事情。

| 上下文类型 | 你控制什么 | 瞬态/持久 |
|------------|------------|-----------|
| 模型上下文 | 模型调用的输入（指令、消息历史、工具、响应格式） | 瞬态 |
| 工具上下文 | 工具能访问和产生什么（读写 state、store、runtime context） | 持久 |
| 生命周期上下文 | 模型和工具调用之间发生什么（总结、护栏、日志等） | 持久 |

### 瞬态 vs 持久

| 类型 | 说明 |
|------|------|
| 瞬态上下文 | LLM 单次调用看到的内容。可以修改消息、工具或提示词，但不改变 state 中保存的内容 |
| 持久上下文 | 跨轮次保存在 state 中的内容。生命周期钩子和工具写入会永久修改它 |

## 数据源

Agent 在执行过程中访问（读/写）不同的数据源：

| 数据源 | 别名 | 作用域 | 示例 |
|--------|------|--------|------|
| Runtime Context | 静态配置 | 对话级 | 用户 ID、API 密钥、数据库连接、权限 |
| State | 短期记忆 | 对话级 | 当前消息、上传文件、认证状态、工具结果 |
| Store | 长期记忆 | 跨对话 | 用户偏好、提取的洞察、历史数据 |

## 模型上下文

控制每次模型调用的输入——指令、可用工具、使用哪个模型、输出格式。

### 系统提示词

系统提示词设置 LLM 的行为和能力。不同用户、上下文或对话阶段需要不同的指令。

**从 State 读取：**

```python
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def state_aware_prompt(request: ModelRequest) -> str:
    message_count = len(request.messages)  # 从 state 读取消息数量
    
    base = "你是一个有帮助的助手。"
    if message_count > 10:
        base += "\n这是一个长对话——请更简洁。"
    
    return base

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[state_aware_prompt]
)
```

**从 Store 读取：**

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langgraph.store.memory import InMemoryStore

@dataclass
class Context:
    user_id: str

@dynamic_prompt
def store_aware_prompt(request: ModelRequest) -> str:
    user_id = request.runtime.context.user_id
    
    # 从 Store 读取用户偏好
    store = request.runtime.store
    user_prefs = store.get(("preferences",), user_id)
    
    base = "你是一个有帮助的助手。"
    if user_prefs:
        style = user_prefs.value.get("communication_style", "balanced")
        base += f"\n用户偏好 {style} 风格的回复。"
    
    return base

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[store_aware_prompt],
    context_schema=Context,
    store=InMemoryStore()
)
```

**从 Runtime Context 读取：**

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dataclass
class Context:
    user_role: str
    deployment_env: str

@dynamic_prompt
def context_aware_prompt(request: ModelRequest) -> str:
    user_role = request.runtime.context.user_role
    env = request.runtime.context.deployment_env
    
    base = "你是一个有帮助的助手。"
    
    if user_role == "admin":
        base += "\n你有管理员权限，可以执行所有操作。"
    elif user_role == "viewer":
        base += "\n你只有只读权限，引导用户使用只读操作。"
    
    if env == "production":
        base += "\n对任何数据修改要格外小心。"
    
    return base

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[context_aware_prompt],
    context_schema=Context
)
```

### 消息

消息组成发送给 LLM 的提示词。管理消息内容对于确保 LLM 有正确的信息来响应至关重要。

**注入文件上下文（从 State）：**

```python
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def inject_file_context(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """注入用户上传的文件上下文"""
    
    # 从 State 读取上传的文件
    uploaded_files = request.state.get("uploaded_files", [])
    
    if uploaded_files:
        file_descriptions = [
            f"- {f['name']} ({f['type']}): {f['summary']}"
            for f in uploaded_files
        ]
        
        file_context = f"""你可以访问以下文件：
{chr(10).join(file_descriptions)}
回答问题时请参考这些文件。"""
        
        # 注入文件上下文
        messages = [
            *request.messages,
            {"role": "user", "content": file_context},
        ]
        request = request.override(messages=messages)
    
    return handler(request)

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[inject_file_context]
)
```

> **瞬态 vs 持久消息更新：**
> 上面的例子使用 `wrap_model_call` 进行**瞬态**更新——修改发送给模型的消息，但不改变 state 中保存的内容。
> 对于**持久**更新（如总结），使用 `before_model` 或 `after_model` 钩子永久更新对话历史。

### 工具

工具让模型与数据库、API 和外部系统交互。如何定义和选择工具直接影响模型能否有效完成任务。

**定义工具：**

```python
from langchain.tools import tool

@tool(parse_docstring=True)
def search_orders(
    user_id: str,
    status: str,
    limit: int = 10
) -> str:
    """按状态搜索用户订单。
    
    当用户询问订单历史或想检查订单状态时使用此工具。
    始终按提供的状态过滤。
    
    Args:
        user_id: 用户的唯一标识符
        status: 订单状态：'pending'、'shipped' 或 'delivered'
        limit: 返回结果的最大数量
    """
    # 实现...
    pass
```

**动态选择工具（基于权限）：**

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@dataclass
class Context:
    user_role: str

@wrap_model_call
def context_based_tools(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """根据用户权限过滤工具"""
    
    user_role = request.runtime.context.user_role
    
    if user_role == "admin":
        # 管理员获得所有工具
        pass
    elif user_role == "editor":
        # 编辑者不能删除
        tools = [t for t in request.tools if t.name != "delete_data"]
        request = request.override(tools=tools)
    else:
        # 查看者只能读取
        tools = [t for t in request.tools if t.name.startswith("read_")]
        request = request.override(tools=tools)
    
    return handler(request)

agent = create_agent(
    model="gpt-4o",
    tools=[read_data, write_data, delete_data],
    middleware=[context_based_tools],
    context_schema=Context
)
```

### 模型

不同模型有不同的优势、成本和上下文窗口。根据任务选择合适的模型。

**根据对话长度选择模型：**

```python
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

# 预先初始化模型
large_model = init_chat_model("claude-sonnet-4-5-20250929")
standard_model = init_chat_model("gpt-4o")
efficient_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def state_based_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """根据对话长度选择模型"""
    
    message_count = len(request.messages)
    
    if message_count > 20:
        # 长对话 - 使用大上下文窗口模型
        model = large_model
    elif message_count > 10:
        # 中等对话
        model = standard_model
    else:
        # 短对话 - 使用高效模型
        model = efficient_model
    
    request = request.override(model=model)
    return handler(request)

agent = create_agent(
    model="gpt-4o-mini",
    tools=[...],
    middleware=[state_based_model]
)
```

### 响应格式

结构化输出将非结构化文本转换为经过验证的结构化数据。

**根据对话阶段选择格式：**

```python
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from pydantic import BaseModel, Field
from typing import Callable

class SimpleResponse(BaseModel):
    """早期对话的简单响应"""
    answer: str = Field(description="简短回答")

class DetailedResponse(BaseModel):
    """成熟对话的详细响应"""
    answer: str = Field(description="详细回答")
    reasoning: str = Field(description="推理解释")
    confidence: float = Field(description="置信度 0-1")

@wrap_model_call
def state_based_output(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """根据对话阶段选择输出格式"""
    
    message_count = len(request.messages)
    
    if message_count < 3:
        # 早期对话 - 简单格式
        request = request.override(response_format=SimpleResponse)
    else:
        # 成熟对话 - 详细格式
        request = request.override(response_format=DetailedResponse)
    
    return handler(request)

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[state_based_output]
)
```

## 工具上下文

工具既读取又写入上下文。

### 读取

大多数实际工具需要的不仅仅是 LLM 的参数。它们需要用户 ID 来查询数据库、API 密钥来访问外部服务，或当前会话状态来做决策。

**从 Runtime Context 读取：**

```python
from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent

@dataclass
class Context:
    user_id: str
    api_key: str
    db_connection: str

@tool
def fetch_user_data(
    query: str,
    runtime: ToolRuntime[Context]
) -> str:
    """使用 Runtime Context 配置获取数据"""
    
    # 从 Runtime Context 读取
    user_id = runtime.context.user_id
    api_key = runtime.context.api_key
    db_connection = runtime.context.db_connection
    
    # 使用配置获取数据
    results = perform_database_query(db_connection, query, api_key)
    return f"为用户 {user_id} 找到 {len(results)} 条结果"

agent = create_agent(
    model="gpt-4o",
    tools=[fetch_user_data],
    context_schema=Context
)

# 调用时传入 context
result = agent.invoke(
    {"messages": [{"role": "user", "content": "获取我的数据"}]},
    context=Context(
        user_id="user_123",
        api_key="sk-...",
        db_connection="postgresql://..."
    )
)
```

### 写入

工具可以返回结果给模型，也可以更新 Agent 的记忆，使重要上下文对未来步骤可用。

**写入 State：**

```python
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent
from langgraph.types import Command

@tool
def authenticate_user(
    password: str,
    runtime: ToolRuntime
) -> Command:
    """认证用户并更新 State"""
    
    if password == "correct":
        # 写入 State：标记为已认证
        return Command(update={"authenticated": True})
    else:
        return Command(update={"authenticated": False})

agent = create_agent(
    model="gpt-4o",
    tools=[authenticate_user]
)
```

**写入 Store：**

```python
from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent
from langgraph.store.memory import InMemoryStore

@dataclass
class Context:
    user_id: str

@tool
def save_preference(
    preference_key: str,
    preference_value: str,
    runtime: ToolRuntime[Context]
) -> str:
    """保存用户偏好到 Store"""
    
    user_id = runtime.context.user_id
    store = runtime.store
    
    # 读取现有偏好
    existing_prefs = store.get(("preferences",), user_id)
    prefs = existing_prefs.value if existing_prefs else {}
    
    # 更新偏好
    prefs[preference_key] = preference_value
    
    # 写入 Store
    store.put(("preferences",), user_id, prefs)
    
    return f"已保存偏好: {preference_key} = {preference_value}"

agent = create_agent(
    model="gpt-4o",
    tools=[save_preference],
    context_schema=Context,
    store=InMemoryStore()
)
```

## 生命周期上下文

控制核心 Agent 步骤**之间**发生的事情——拦截数据流以实现跨切面关注点，如总结、护栏和日志。

中间件允许你钩入 Agent 生命周期的任何步骤，并：
1. **更新上下文** - 修改 state 和 store 以持久化更改
2. **跳转生命周期** - 根据上下文移动到 Agent 循环的不同步骤

### 示例：总结

最常见的生命周期模式之一是当对话历史太长时自动压缩。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger=("tokens", 4000),
            keep=("messages", 20),
        ),
    ],
)
```

当对话超过 token 限制时，`SummarizationMiddleware` 自动：
1. 使用单独的 LLM 调用总结旧消息
2. 用摘要消息替换它们（永久更新 State）
3. 保持最近的消息完整

## 最佳实践

1. **从简单开始** - 先用静态提示词和工具，需要时再添加动态功能
2. **增量测试** - 一次添加一个上下文工程特性
3. **监控性能** - 跟踪模型调用、token 使用和延迟
4. **使用内置中间件** - 利用 `SummarizationMiddleware`、`LLMToolSelectorMiddleware` 等
5. **记录上下文策略** - 明确传递了什么上下文以及为什么
6. **理解瞬态 vs 持久** - 模型上下文更改是瞬态的（每次调用），生命周期上下文更改持久化到 state

## 总结

| 概念 | 说明 |
|------|------|
| 上下文工程 | 以正确格式提供正确信息，让 LLM 完成任务 |
| 模型上下文 | 系统提示词、消息、工具、模型、响应格式 |
| 工具上下文 | 工具读写的数据（state、store、runtime context） |
| 生命周期上下文 | 步骤之间的处理（总结、护栏、日志） |
| 瞬态上下文 | 单次调用的修改，不保存 |
| 持久上下文 | 永久保存到 state 的修改 |
| Runtime Context | 静态配置（用户 ID、API 密钥等） |
| State | 短期记忆（对话级） |
| Store | 长期记忆（跨对话） |
